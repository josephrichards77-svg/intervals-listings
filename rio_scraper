import html, os, re, csv, sqlite3
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import requests
import dateparser
from playwright.sync_api import sync_playwright

# Cleaner tools (assuming these are shared across projects)
from intervals_cleaning import normalise_row, merge_rows, export_rows 

# ============================================================
# CONFIG
# ============================================================
CINEMA_NAME = "The Rio"
BASE_URL = "https://riocinema.org.uk"
URL = f"{BASE_URL}/Rio.dll/WhatsOn"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b" # Must be same as other scrapers

# Shared database file
DB_PATH = Path(__file__).with_name("tmdb_cache.db") 
_MEM_CACHE = {}

# ============================================================
# PREFIX CLEANING (RIO)
# ============================================================
PREFIXES = [
    "Pink Palace:", "Film Club:", "Screentalk:", "ScreenTalk:",
    "Q&A:", "Q & A:", "Special Screening:", "Cult Classic:",
    "Restoration:", "Preview:", "Introduction:", "Intro:",
    "Late Night:", "Matinee:",
]

def strip_prefixes(title):
    t = title.strip()
    found = []
    changed = True
    while changed:
        changed = False
        for p in PREFIXES:
            if t.lower().startswith(p.lower()):
                found.append(p.rstrip(":"))
                t = t[len(p):].strip()
                changed = True
    return t, found

def normalize_url(h):
    if not h: return ""
    if h.startswith(("http://","https://")): return h
    if h.startswith("/"): return BASE_URL + h
    return BASE_URL + "/" + h

def strip_rating(t):
    # Rio ratings are usually outside the main title, but keep this safety check
    return re.sub(r"\b(U|PG|12A|12|15|18)\b$", "", t).strip()

# ============================================================
# TMDB CACHE (UNIFIED LOGIC)
# ============================================================

def init_db():
    """Initializes DB with the unified (title, year_hint) schema."""
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS tmdb_cache (
            title TEXT NOT NULL,
            year_hint TEXT,
            tmdb_id INTEGER,
            runtime_min INTEGER,
            director TEXT,
            final_year TEXT,
            last_updated TEXT,
            PRIMARY KEY (title, year_hint)
        )
    """)
    conn.commit()
    return conn

# NOTE: This tmdb_get_metadata is the unified version from your PCC scraper.
def tmdb_get_metadata(conn, title: str, year_hint: str | None):
    """Fetches TMDB metadata using the unified cache structure."""
    key = (title, year_hint or "")
    
    if key in _MEM_CACHE: return _MEM_CACHE[key]

    cur = conn.execute(
        "SELECT runtime_min, director, final_year FROM tmdb_cache WHERE title = ? AND year_hint IS ?",
        (title, year_hint),
    )
    row = cur.fetchone()
    if row:
        runtime_min, director, final_year = row
        meta = {
            "runtime_min": runtime_min,
            "director": director,
            "year": final_year or year_hint,
        }
        _MEM_CACHE[key] = meta
        return meta

    if not TMDB_API_KEY:
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta

    try:
        # --- Search by title (and optional year_hint) ---
        params = {"api_key": TMDB_API_KEY, "query": title}
        if year_hint: params["year"] = year_hint

        r = requests.get(
            "https://api.themoviedb.org/3/search/movie", params=params, timeout=10
        )
        r.raise_for_status()
        results = r.json().get("results") or []

        if not results:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        movie = results[0]
        # Year matching logic (copied from your PCC logic)
        if year_hint:
            for m in results:
                rd = (m.get("release_date") or "")[:4]
                if rd == str(year_hint):
                    movie = m; break

        movie_id = movie.get("id")
        if not movie_id:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        # --- Get details + credits ---
        params = {"api_key": TMDB_API_KEY, "append_to_response": "credits"}
        r2 = requests.get(
            f"https://api.themoviedb.org/3/movie/{movie_id}",
            params=params, timeout=10
        )
        r2.raise_for_status()
        mdata = r2.json()

        runtime_min = mdata.get("runtime")
        release_date = mdata.get("release_date") or ""
        tmdb_year = release_date[:4] if release_date else None
        final_year = tmdb_year or year_hint

        director = None
        credits = (mdata.get("credits") or {}).get("crew") or []
        for c in credits:
            if c.get("job") == "Director":
                director = c.get("name"); break

        meta = {
            "runtime_min": runtime_min,
            "director": director or "",
            "year": final_year or "",
        }

        # Store in SQLite (using the unified schema)
        conn.execute("""
            INSERT OR REPLACE INTO tmdb_cache
            (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            title, year_hint, movie_id, runtime_min, director,
            final_year, datetime.utcnow().isoformat()
        ))
        conn.commit()

        _MEM_CACHE[key] = meta
        return meta

    except Exception:
        # Fallback on any error
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta

# ============================================================
# SCRAPER CORE (Uses unified TMDB functions)
# ============================================================
def scrape():
    """Scrapes Rio using the internal JavaScript data feed."""
    out = []
    conn = init_db() # Initialize unified DB

    with sync_playwright() as p:
        b = p.chromium.launch(headless=True)
        page = b.new_page()

        print("Loading Rio feed...")
        try:
            # We don't use networkidle here because we are reading a JS variable
            page.goto(URL, timeout=60000)
        except Exception as e:
            print(f"Error loading Rio page: {e}")
            b.close()
            conn.close()
            return []

        # Access the JavaScript variable 'window.Events'
        data = page.evaluate("window.Events || null")
        if not data:
            print("Rio feed missing 'Events' data.")
            b.close()
            conn.close()
            return []

        for e in data.get("Events", []):
            raw_title = html.unescape(e["Title"])
            
            # Use a year_hint of None since Rio's feed doesn't easily provide one
            year_hint = None 

            # Clean prefixes and rating
            t1 = strip_rating(raw_title) 
            title, prefixes = strip_prefixes(t1)
            notes = "; ".join(prefixes)
            
            # --- TMDB Lookup ---
            tmdb = tmdb_get_metadata(conn, title, year_hint)
            # -------------------
            
            director = tmdb.get("director", "")
            runtime = tmdb.get("runtime_min", "")
            final_year = tmdb.get("year", "")
            
            link = normalize_url(e["URL"])
            
            # Presume DCP/Digital as a default format
            default_format = "DCP" 

            for perf in e["Performances"]:
                dt_str = f"{perf['StartDate']} {perf['StartTime']}"
                dt = datetime.datetime.strptime(dt_str, "%Y-%m-%d %H%M")
                
                # Check if the screening date is in the past (optional cleanup)
                if dt < datetime.datetime.now():
                    continue

                out.append({
                    "title": title,
                    "link": link,
                    "datetime": dt,
                    "director": director,
                    "runtime_min": runtime,
                    "year": final_year,
                    "extra": notes,
                })

        b.close()
        conn.close()

    print(f"Scraped {len(out)} Rio showtimes")
    return out

# ============================================================
# CONVERT → INTERVALS ROWS
# ============================================================
def convert_to_rows(showtimes):
    rows = []
    default_format = "DCP" 

    per_date = defaultdict(list)
    # Group by (title, date)
    for s in showtimes:
        key = (s["title"], s["datetime"].strftime("%Y-%m-%d"))
        per_date[key].append(s)

    for (title, date_str), listings in per_date.items():
        # All listings for the same title/date have the same metadata
        first = listings[0]
        
        times = sorted([l["datetime"].strftime("%H:%M") for l in listings])

        raw = {
            "venue": CINEMA_NAME,
            "date": date_str,
            "title": title,
            "director": first.get("director",""),
            "runtime_min": first.get("runtime_min",""),
            "format": default_format,
            "times": times,
            "year": first.get("year",""),
            "extra": first.get("extra",""),
            "url": first.get("link",""),
        }

        normal = normalise_row(raw)

        # Apply hyperlink AFTER normalise_row
        normal["title"] = f'<a href="{normal["url"]}" target="_blank">{normal["title"]}</a>'

        merge_rows(rows, normal)

    return rows

# ============================================================
# INCREMENTAL SAVE + BACKUP
# ============================================================
# (Keeping your original incremental_save function here for completeness)

def incremental_save(rows, name="rio"):
    # NOTE: This function requires 're', 'Path', and 'datetime' from imports
    def add_spaces(line):
        return re.sub(r";(?=\S)", "; ", line)

    exported = export_rows(rows)
    exported = add_spaces(exported)
    lines = exported.split("\n")

    base = Path(__file__)
    existing_path = base.with_name(f"{name}_existing.txt")
    backup_dir = base.with_name(f"{name}_backups")

    if existing_path.exists():
        old = add_spaces(existing_path.read_text())
        existing = {l.strip() for l in old.split("\n") if l.strip()}
    else:
        existing = set()

    new = [l for l in lines if l.strip() and l not in existing]

    updated = existing.union(new)
    existing_path.write_text("\n".join(sorted(updated)))

    backup_dir.mkdir(exist_ok=True)
    ts = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup = backup_dir / f"{name}_{ts}.txt"
    backup.write_text(exported)

    print(f"Backup saved → {backup}")
    return new

# ============================================================
# MAIN
# ============================================================
if __name__ == "__main__":
    # ⚠️ REMINDER: Delete your existing tmdb_cache.db file once before running 
    # this script and your other scrapers to start fresh with the unified schema.
    print("--- Starting Rio Scraper ---")
    showtimes = scrape()
    rows = convert_to_rows(showtimes)
    new = incremental_save(rows, "rio")
    
    print("\n--- NEW RIO LISTINGS ---")
    print("\n".join(sorted(new)))
