import html, os, re, csv, sqlite3, datetime, requests
from pathlib import Path
from playwright.sync_api import sync_playwright
from dataclasses import dataclass, field
from intervals_cleaning import normalise_row, merge_rows, export_rows

# ============================================================
# CONFIG
# ============================================================
CINEMA_NAME = "The Rio"
BASE_URL = "https://riocinema.org.uk"
URL = f"{BASE_URL}/Rio.dll/WhatsOn"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
TMDB_DB_FILENAME = "tmdb_cache.db"
DATE_FMT = "%Y-%m-%d"
TIME_FMT = "%H:%M"
session = requests.Session()

# ============================================================
# HELPERS
# ============================================================
def normalize_url(h):
    if not h: return ""
    if h.startswith(("http://","https://")): return h
    if h.startswith("/"): return BASE_URL + h
    return BASE_URL + "/" + h

def strip_rating(t):
    return re.sub(r"\b(U|PG|12A|12|15|18)\b$", "", t).strip()

# Strict, explicit prefix list only
PREFIXES = [
    "Pink Palace:",
    "Film Club:",
    "Screentalk:",
    "ScreenTalk:",
    "Q&A:",
    "Q & A:",
    "Special Screening:",
    "Cult Classic:",
    "Restoration:",
    "Preview:",
    "Introduction:",
    "Intro:",
    "Late Night:",
    "Matinee:",
]

def strip_prefixes(title):
    """
    Removes only explicit prefixes defined in PREFIXES.
    Case-insensitive. Supports multiple stacked prefixes.
    """
    t = title.strip()
    found = []
    changed = True

    while changed:
        changed = False
        for p in PREFIXES:
            if t.lower().startswith(p.lower()):
                found.append(p.rstrip(":"))
                t = t[len(p):].strip()
                changed = True

    return t, found

# ============================================================
# TMDB CACHE
# ============================================================
_tmdb_conn = None

def get_tmdb_conn():
    global _tmdb_conn
    db_path = os.path.join(os.path.dirname(__file__), TMDB_DB_FILENAME)

    if _tmdb_conn is None:
        _tmdb_conn = sqlite3.connect(db_path)
        _tmdb_conn.row_factory = sqlite3.Row
        cur = _tmdb_conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS tmdb_cache (
                title TEXT PRIMARY KEY,
                tmdb_id INTEGER,
                runtime_min INTEGER,
                director TEXT,
                final_year INTEGER,
                last_updated TEXT
            )""")
        _tmdb_conn.commit()

    return _tmdb_conn

def tmdb_cache_lookup(title):
    cur = get_tmdb_conn().cursor()
    cur.execute("SELECT * FROM tmdb_cache WHERE LOWER(title)=LOWER(?)", (title,))
    row = cur.fetchone()
    return dict(row) if row else {}

def tmdb_cache_upsert(title, tmdb_id, runtime, director, year):
    conn = get_tmdb_conn()
    ts = datetime.datetime.now(datetime.UTC).isoformat(timespec="seconds")
    conn.execute("""
        INSERT INTO tmdb_cache (title, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?)
        ON CONFLICT(title) DO UPDATE SET
            tmdb_id=excluded.tmdb_id,
            runtime_min=excluded.runtime_min,
            director=excluded.director,
            final_year=excluded.final_year,
            last_updated=excluded.last_updated
    """, (title, tmdb_id, runtime, director, year, ts))
    conn.commit()

def tmdb_fetch(title):
    if not TMDB_API_KEY:
        return {}

    try:
        res = session.get(
            "https://api.themoviedb.org/3/search/movie",
            params={"api_key": TMDB_API_KEY, "query": title},
            timeout=10
        ).json()
    except:
        return {}

    results = res.get("results") or []
    if not results:
        return {}

    movie = max(results, key=lambda r: r.get("popularity") or 0)
    tmdb_id = movie["id"]

    det = session.get(
        f"https://api.themoviedb.org/3/movie/{tmdb_id}",
        params={"api_key": TMDB_API_KEY},
        timeout=10
    ).json()

    runtime = det.get("runtime")
    rd = det.get("release_date") or ""
    year = int(rd[:4]) if len(rd) >= 4 else None

    credits = session.get(
        f"https://api.themoviedb.org/3/movie/{tmdb_id}/credits",
        params={"api_key": TMDB_API_KEY},
        timeout=10
    ).json()

    director = ""
    for c in credits.get("crew", []):
        if c.get("job") == "Director":
            director = c["name"]
            break

    tmdb_cache_upsert(title, tmdb_id, runtime, director, year)

    return {
        "director": director or "",
        "runtime": runtime or "",
        "year": year or ""
    }

def lookup_tmdb(title):
    c = tmdb_cache_lookup(title)
    d = c.get("director") or ""
    r = c.get("runtime_min") or ""
    y = c.get("final_year") or ""

    if not (d and r and y):
        f = tmdb_fetch(title)
        d = f.get("director", d)
        r = f.get("runtime", r)
        y = f.get("year", y)

    return {
        "director": d,
        "runtime": str(r) if r else "",
        "year": y,
        "format": "DCP"
    }

# ============================================================
# DATA STRUCTURE
# ============================================================
@dataclass
class ShowTime:
    title: str
    link: str
    datetime: datetime.datetime
    description: str
    img: str
    extra: dict
    notes: str = ""

# ============================================================
# SCRAPER
# ============================================================
def scrape():
    out = []

    with sync_playwright() as p:
        b = p.chromium.launch(headless=True)
        page = b.new_page()
        page.goto(URL)

        data = page.evaluate("window.Events || null")
        if not data:
            raise RuntimeError("Rio feed missing 'Events'")

        for e in data["Events"]:
            raw = html.unescape(e["Title"])
            t1 = strip_rating(raw)
            title, prefixes = strip_prefixes(t1)

            tmdb = lookup_tmdb(title)

            link = normalize_url(e["URL"])
            desc = html.unescape(e["Synopsis"])
            img = normalize_url(e["ImageURL"])

            for perf in e["Performances"]:
                dt = datetime.datetime.strptime(
                    f"{perf['StartDate']} {perf['StartTime']}",
                    "%Y-%m-%d %H%M"
                )

                out.append(ShowTime(
                    title=title,
                    link=link,
                    datetime=dt,
                    description=desc,
                    img=img,
                    extra=tmdb,
                    notes="; ".join(prefixes)
                ))

        b.close()

    print(f"Scraped {len(out)} Rio showtimes")
    return out

# ============================================================
# CONVERT → INTERVALS ROWS
# ============================================================
def convert_to_rows(showtimes):
    rows = []

    for s in showtimes:
        raw = {
            "venue": CINEMA_NAME,
            "date": s.datetime.strftime(DATE_FMT),
            "title": s.title,   # CLEAN title ("Halston")
            "director": s.extra.get("director",""),
            "runtime_min": s.extra.get("runtime",""),
            "format": s.extra.get("format","DCP"),
            "times": [s.datetime.strftime(TIME_FMT)],
            "year": s.extra.get("year",""),
            "extra": s.notes,
            "url": s.link,
        }

        normal = normalise_row(raw)

        # FORCE CLEAN TITLE
        normal["title"] = s.title
        normal["title"] = f'<a href="{normal["url"]}" target="_blank">{normal["title"]}</a>'

        merge_rows(rows, normal)

    return rows

# ============================================================
# INCREMENTAL SAVE + BACKUP
# ============================================================
def incremental_save(rows, name="rio"):

    def add_spaces(line):
        return re.sub(r";(?=\S)", "; ", line)

    exported = export_rows(rows)
    exported = add_spaces(exported)
    lines = exported.split("\n")

    base = Path(__file__)
    existing_path = base.with_name(f"{name}_existing.txt")
    backup_dir = base.with_name(f"{name}_backups")

    if existing_path.exists():
        old = add_spaces(existing_path.read_text())
        existing = {l.strip() for l in old.split("\n") if l.strip()}
    else:
        existing = set()

    new = [l for l in lines if l.strip() and l not in existing]

    updated = existing.union(new)
    existing_path.write_text("\n".join(sorted(updated)))

    backup_dir.mkdir(exist_ok=True)
    ts = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup = backup_dir / f"{name}_{ts}.txt"
    backup.write_text(exported)

    print(f"Backup saved → {backup}")
    return new

# ============================================================
# MAIN
# ============================================================
if __name__ == "__main__":
    showtimes = scrape()
    rows = convert_to_rows(showtimes)
    new = incremental_save(rows, "rio")
    print("\n".join(sorted(new)))
