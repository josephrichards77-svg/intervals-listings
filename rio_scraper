import html
import os
import re
import csv
import sqlite3
import datetime
import requests

from pathlib import Path
from playwright.sync_api import sync_playwright
from dataclasses import dataclass, field

# ============================================================
# CONFIG
# ============================================================

CINEMA_NAME = "The Rio"
CINEMA_SHORTCODE = "RI"
BASE_URL = "https://riocinema.org.uk"
URL = f"{BASE_URL}/Rio.dll/WhatsOn"

TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
TMDB_DB_FILENAME = "tmdb_cache.db"

DATE_FMT = "%Y-%m-%d"
TIME_FMT = "%H:%M"

# Persistent HTTP session
session = requests.Session()


# ============================================================
# HELPERS
# ============================================================

def normalize_url(href: str) -> str:
    if not href:
        return ""
    if href.startswith(("http://", "https://")):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return BASE_URL + "/" + href


def strip_rating(title: str) -> str:
    """
    Removes BBFC ratings like:
    U, PG, 12A, 12, 15, 18
    and variants like "12A Cert" or "(15)" or trailing parentheses.
    """
    rating_re = re.compile(r"\b(U|PG|12A|12|15|18)\s*(Cert)?\)?$", re.IGNORECASE)
    cleaned = rating_re.sub("", title).strip()
    return re.sub(r"\s{2,}", " ", cleaned)


# ============================================================
# TMDB CACHE + LOOKUP
# ============================================================

_tmdb_conn = None

def get_tmdb_conn():
    """
    Ensures SQLite file and table exist.
    """
    global _tmdb_conn
    db_path = os.path.join(os.path.dirname(__file__), TMDB_DB_FILENAME)

    if _tmdb_conn is None:
        first_time = not os.path.exists(db_path)
        _tmdb_conn = sqlite3.connect(db_path)
        _tmdb_conn.row_factory = sqlite3.Row

        # Ensure schema exists
        cur = _tmdb_conn.cursor()
        cur.execute("""
            CREATE TABLE IF NOT EXISTS tmdb_cache (
                title TEXT PRIMARY KEY,
                tmdb_id INTEGER,
                runtime_min INTEGER,
                director TEXT,
                final_year INTEGER,
                last_updated TEXT
            )
        """)
        _tmdb_conn.commit()

    return _tmdb_conn


def tmdb_cache_lookup(title_clean):
    conn = get_tmdb_conn()
    cur = conn.cursor()
    cur.execute("SELECT * FROM tmdb_cache WHERE LOWER(title)=LOWER(?)", (title_clean,))
    row = cur.fetchone()
    return dict(row) if row else {}


def tmdb_cache_upsert(title, tmdb_id, runtime, director, year):
    conn = get_tmdb_conn()
    cur = conn.cursor()
    ts = datetime.datetime.now(datetime.UTC).isoformat(timespec="seconds")

    cur.execute("""
        INSERT INTO tmdb_cache (title, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?)
        ON CONFLICT(title) DO UPDATE SET
            tmdb_id=excluded.tmdb_id,
            runtime_min=excluded.runtime_min,
            director=excluded.director,
            final_year=excluded.final_year,
            last_updated=excluded.last_updated
    """, (title, tmdb_id, runtime, director, year, ts))

    conn.commit()


def tmdb_fetch(title_clean):
    if not TMDB_API_KEY:
        return {}

    try:
        r = session.get(
            "https://api.themoviedb.org/3/search/movie",
            params={"api_key": TMDB_API_KEY, "query": title_clean},
            timeout=10
        )
        r.raise_for_status()
        s = r.json()
    except Exception:
        return {}

    results = s.get("results") or []
    if not results:
        return {}

    # Choose *most popular* result instead of first
    best = max(results, key=lambda r: r.get("popularity") or 0)
    tmdb_id = best["id"]

    # details
    try:
        det = session.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}",
            params={"api_key": TMDB_API_KEY},
            timeout=10
        ).json()
    except Exception:
        return {}

    runtime = det.get("runtime")
    release_date = det.get("release_date") or ""
    year = int(release_date[:4]) if len(release_date) >= 4 else None

    # director
    director = ""
    try:
        credits = session.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}/credits",
            params={"api_key": TMDB_API_KEY},
            timeout=10
        ).json()

        for c in credits.get("crew", []):
            if c.get("job") == "Director":
                director = c["name"]
                break
    except Exception:
        pass

    tmdb_cache_upsert(title_clean, tmdb_id, runtime, director, year)

    return {
        "director": director or "",
        "runtime": runtime or "",
        "year": year or "",
    }


def lookup_tmdb(title_clean):
    cached = tmdb_cache_lookup(title_clean)

    director = cached.get("director") or ""
    runtime = cached.get("runtime_min") or ""
    year = cached.get("final_year") or ""

    if not (director and runtime and year):
        fresh = tmdb_fetch(title_clean)
        if fresh:
            director = fresh["director"] or director
            runtime = fresh["runtime"] or runtime
            year = fresh["year"] or year

    return {
        "director": director,
        "runtime": str(runtime) if runtime else "",
        "year": year or "",
        "format": "DCP",
    }


# ============================================================
# DATACLASS
# ============================================================

@dataclass
class ShowTime:
    title: str
    link: str
    datetime: datetime.datetime
    description: str
    img: str
    extra: dict = field(default_factory=dict)


# ============================================================
# SCRAPER
# ============================================================

def scrape():
    out = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(URL)

        # Safer extraction of JS variable
        data = page.evaluate("window.Events || null")
        if not data or "Events" not in data:
            browser.close()
            raise RuntimeError("Rio feed missing JS variable 'Events'")

        events = data["Events"]

        for event in events:
            title_raw = html.unescape(event["Title"])
            title = strip_rating(title_raw)
            link = normalize_url(event["URL"])
            description = html.unescape(event["Synopsis"])
            img = normalize_url(event["ImageURL"])

            tmdb = lookup_tmdb(title)

            for perf in event["Performances"]:
                dt_str = f"{perf['StartDate']} {perf['StartTime']}"
                dt = datetime.datetime.strptime(dt_str, "%Y-%m-%d %H%M")

                out.append(
                    ShowTime(
                        title=title,
                        link=link,
                        datetime=dt,
                        description=description,
                        img=img,
                        extra=tmdb,
                    )
                )

        browser.close()

    print(f"Scraped {len(out)} Rio showtimes")
    return out


# ============================================================
# INTERVALS CLEANING + MERGING + EXPORT
# ============================================================

from intervals_cleaning import normalise_row, merge_rows, export_rows

def convert_showtimes_to_intervals_rows(showtimes):
    rows = []

    for s in showtimes:
        date_iso = s.datetime.strftime(DATE_FMT)
        time_24  = s.datetime.strftime(TIME_FMT)

        raw = {
            "venue": CINEMA_NAME,
            "date": date_iso,
            "title": s.title,
            "director": s.extra.get("director", ""),
            "runtime_min": s.extra.get("runtime", ""),
            "format": s.extra.get("format", "DCP"),
            "times": [time_24],
            "year": s.extra.get("year", ""),
            "extra": "",
            "url": s.link,
        }

        normal = normalise_row(raw)

        clean_title = normal["title"]
        clean_url   = normal["url"]
        normal["title"] = f'<a href="{clean_url}" target="_blank">{clean_title}</a>'

        merge_rows(rows, normal)

    return rows


def incremental_save_and_backup(rows, scraper_name="rio"):
    exported = export_rows(rows)
    lines = exported.split("\n")

    base = Path(__file__)
    existing_path = base.with_name(f"{scraper_name}_existing.txt")
    backup_dir = base.with_name(f"{scraper_name}_backups")

    if existing_path.exists():
        existing = {line.strip() for line in existing_path.read_text().split("\n") if line.strip()}
    else:
        existing = set()

    new_rows = [r for r in lines if r not in existing]

    updated = existing.union(new_rows)
    with open(existing_path, "w", encoding="utf-8") as f:
        for r in sorted(updated):
            f.write(r + "\n")

    backup_dir.mkdir(exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup_path = backup_dir / f"{scraper_name}_{timestamp}.txt"

    # Store full snapshot, not only new rows
    with open(backup_path, "w", encoding="utf-8") as f:
        f.write(exported)

    print(f"\nBackup saved → {backup_path}")

    return new_rows


# ============================================================
# EXPORT
# ============================================================

def export_to_semicolon(showtimes, filename="rio_semicolon.txt"):
    grouped = {}

    for s in showtimes:
        date = s.datetime.strftime(DATE_FMT)
        time = s.datetime.strftime(TIME_FMT)
        key = (s.title, date)

        if key not in grouped:
            grouped[key] = {
                "date": date,
                "title": s.title,
                "link": s.link,
                "director": s.extra.get("director", ""),
                "runtime": s.extra.get("runtime", ""),
                "format": s.extra.get("format", "DCP"),
                "year": s.extra.get("year", ""),
                "times": [],
            }

        grouped[key]["times"].append(time)

    with open(filename, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter=";")

        writer.writerow(["DATE", "VENUE", "TITLE", "DIRECTOR", "RUNTIME", "FORMAT", "TIME", "YEAR"])

        # stable output ordering
        for _, data in sorted(grouped.items(), key=lambda x: (x[1]["date"], x[1]["title"])):
            times = ",".join(sorted(set(data["times"])))
            safe_link = normalize_url(data["link"])
            title_html = f'<a href="{safe_link}">{data["title"]}</a>'

            writer.writerow([
                data["date"],
                CINEMA_NAME,
                title_html,
                data["director"],
                data["runtime"],
                data["format"],
                times,
                data["year"],
            ])

    print(f"✔ Exported {len(grouped)} Rio rows → {filename}")


# ============================================================
# RUN
# ============================================================

if __name__ == "__main__":
    showtimes = scrape()
    rows = convert_showtimes_to_intervals_rows(showtimes)
    new_rows = incremental_save_and_backup(rows, "rio")
    print("\n".join(new_rows))
