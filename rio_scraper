import html
import os
import re
import csv
import sqlite3
import datetime
import requests

from playwright.sync_api import sync_playwright
from dataclasses import dataclass, field

# ============================================================
# CONFIG
# ============================================================

CINEMA_NAME = "The Rio"
CINEMA_SHORTCODE = "RI"
BASE_URL = "https://riocinema.org.uk"
URL = f"{BASE_URL}/Rio.dll/WhatsOn"

TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
TMDB_DB_FILENAME = "tmdb_cache.db"


# ============================================================
# HELPERS
# ============================================================

def normalize_url(href: str) -> str:
    if not href:
        return ""
    if href.startswith("http://") or href.startswith("https://"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return BASE_URL + "/" + href


def strip_rating(title: str) -> str:
    rating_re = re.compile(r"\b(U|PG|12A|12|15|18)\b$", re.IGNORECASE)
    return rating_re.sub("", title).strip()


# ============================================================
# TMDB CACHE + LOOKUP
# ============================================================

_tmdb_conn = None
def get_tmdb_conn():
    global _tmdb_conn
    db_path = os.path.join(os.path.dirname(__file__), TMDB_DB_FILENAME)
    if not os.path.exists(db_path):
        return None
    if _tmdb_conn is None:
        _tmdb_conn = sqlite3.connect(db_path)
        _tmdb_conn.row_factory = sqlite3.Row
    return _tmdb_conn


def tmdb_cache_lookup(title_clean):
    conn = get_tmdb_conn()
    if not conn:
        return {}

    cur = conn.cursor()
    cur.execute("""
        SELECT * FROM tmdb_cache WHERE LOWER(title)=LOWER(?)
    """, (title_clean,))
    row = cur.fetchone()
    return dict(row) if row else {}


def tmdb_cache_upsert(title, tmdb_id, runtime, director, year):
    conn = get_tmdb_conn()
    if not conn:
        return

    cur = conn.cursor()
    ts = datetime.datetime.now(datetime.UTC).isoformat(timespec="seconds")

    cur.execute("""
        INSERT INTO tmdb_cache (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, NULL, ?, ?, ?, ?, ?)
        ON CONFLICT(title, year_hint) DO UPDATE SET
            tmdb_id=excluded.tmdb_id,
            runtime_min=excluded.runtime_min,
            director=excluded.director,
            final_year=excluded.final_year,
            last_updated=excluded.last_updated
    """, (title, tmdb_id, runtime, director, year, ts))

    conn.commit()


def tmdb_fetch(title_clean):
    if not TMDB_API_KEY:
        return {}

    # search
    s = requests.get(
        "https://api.themoviedb.org/3/search/movie",
        params={"api_key": TMDB_API_KEY, "query": title_clean},
        timeout=10
    ).json()

    results = s.get("results") or []
    if not results:
        return {}

    best = results[0]
    tmdb_id = best["id"]

    # details
    det = requests.get(
        f"https://api.themoviedb.org/3/movie/{tmdb_id}",
        params={"api_key": TMDB_API_KEY},
        timeout=10
    ).json()

    runtime = det.get("runtime")
    release_date = det.get("release_date") or ""
    year = int(release_date[:4]) if len(release_date) >= 4 else None

    # director
    director = ""
    credits = requests.get(
        f"https://api.themoviedb.org/3/movie/{tmdb_id}/credits",
        params={"api_key": TMDB_API_KEY},
        timeout=10
    ).json()

    for c in credits.get("crew", []):
        if c.get("job") == "Director":
            director = c["name"]
            break

    # save
    tmdb_cache_upsert(title_clean, tmdb_id, runtime, director, year)

    return {
        "director": director or "",
        "runtime": runtime or "",
        "year": year or "",
    }


def lookup_tmdb(title_clean):
    cached = tmdb_cache_lookup(title_clean)

    director = cached.get("director") or ""
    runtime = cached.get("runtime_min") or ""
    year = cached.get("final_year") or ""

    if not director or not runtime or not year:
        api = tmdb_fetch(title_clean)
        if api:
            director = api["director"] or director
            runtime = api["runtime"] or runtime
            year = api["year"] or year

    return {
        "director": director,
        "runtime": str(runtime) if runtime else "",
        "year": year or "",
        "format": "DCP",
    }


# ============================================================
# DATACLASS
# ============================================================

@dataclass
class ShowTime:
    title: str
    link: str
    datetime: datetime.datetime
    description: str
    img: str
    extra: dict = field(default_factory=dict)


# ============================================================
# SCRAPER
# ============================================================

def scrape():
    out = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(URL)

        # Rio feed is in a JS variable "Events"
        data = page.evaluate("Events")
        events = data["Events"]

        for event in events:
            title_raw = html.unescape(event["Title"])
            title = strip_rating(title_raw)
            link = normalize_url(event["URL"])
            description = html.unescape(event["Synopsis"])
            img = event["ImageURL"]

            # TMDB enrichment
            tmdb = lookup_tmdb(title)

            for perf in event["Performances"]:
                dt_str = f"{perf['StartDate']} {perf['StartTime']}"
                dt = datetime.datetime.strptime(dt_str, "%Y-%m-%d %H%M")

                out.append(
                    ShowTime(
                        title=title,
                        link=link,
                        datetime=dt,
                        description=description,
                        img=img,
                        extra=tmdb,
                    )
                )

        browser.close()

    print(f"Scraped {len(out)} Rio showtimes")
    return out
# ============================================================
# INTERVALS CLEANING + MERGING + EXPORT
# (Drop-in module — SAFE, does not touch scraper logic)
# ============================================================

from intervals_cleaning import normalise_row, merge_rows, export_rows

def convert_showtimes_to_intervals_rows(showtimes):
    """
    Takes the raw ShowTime objects from scrape()
    and converts to the Intervals unified row schema.
    """

    rows = []

    for s in showtimes:
        date_iso = s.datetime.strftime("%Y-%m-%d")
        time_24  = s.datetime.strftime("%H:%M")

        raw = {
            "venue": CINEMA_NAME,
            "date": date_iso,
            "title": s.title,
            "director": s.extra.get("director", ""),
            "runtime_min": s.extra.get("runtime", ""),
            "format": s.extra.get("format", "DCP"),
            "times": [time_24],
            "year": s.extra.get("year", ""),
            "extra": "",
            "url": s.link,
        }

        normal = normalise_row(raw)

        # Wrap cleaned title in <a>
        clean_title = normal["title"]
        clean_url   = normal["url"]
        normal["title"] = f'<a href="{clean_url}" target="_blank">{clean_title}</a>'

        merge_rows(rows, normal)

    return rows


def incremental_save_and_backup(rows, scraper_name="rio"):
    """
    Exactly like the PCC system:
    - rio_existing.txt
    - rio_backups/
    - only print new rows
    """

    exported = export_rows(rows)
    lines = exported.split("\n")

    base = Path(__file__)

    existing_path = base.with_name(f"{scraper_name}_existing.txt")
    backup_dir    = base.with_name(f"{scraper_name}_backups")

    # Load existing
    if existing_path.exists():
        existing = {line.strip() for line in existing_path.read_text().split("\n") if line.strip()}
    else:
        existing = set()

    # Determine new rows
    new_rows = [r for r in lines if r not in existing]

    # Save updated existing
    updated = existing.union(new_rows)
    with open(existing_path, "w", encoding="utf-8") as f:
        for r in sorted(updated):
            f.write(r + "\n")

    # Backup
    backup_dir.mkdir(exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup_path = backup_dir / f"{scraper_name}_{timestamp}.txt"

    with open(backup_path, "w", encoding="utf-8") as f:
        f.write("\n".join(new_rows))

    print(f"\nBackup saved → {backup_path}")

    return new_rows


# ============================================================
# EXPORT
# ============================================================

def export_to_semicolon(showtimes, filename="rio_semicolon.txt"):
    grouped = {}

    for s in showtimes:
        date = s.datetime.strftime("%Y-%m-%d")
        time = s.datetime.strftime("%H:%M")
        key = (s.title, date)

        if key not in grouped:
            grouped[key] = {
                "date": date,
                "title": s.title,
                "link": s.link,
                "director": s.extra.get("director", ""),
                "runtime": s.extra.get("runtime", ""),
                "format": s.extra.get("format", "DCP"),
                "year": s.extra.get("year", ""),
                "times": [],
            }

        grouped[key]["times"].append(time)

    with open(filename, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter=";")

        writer.writerow([
            "DATE", "VENUE", "TITLE",
            "DIRECTOR", "RUNTIME", "FORMAT",
            "TIME", "YEAR"
        ])

        for k, data in grouped.items():
            times = ",".join(sorted(set(data["times"])))

            safe_link = normalize_url(data["link"])
            title_html = f'<a href="{safe_link}">{data["title"]}</a>'

            writer.writerow([
                data["date"],
                CINEMA_NAME,
                title_html,
                data["director"],
                data["runtime"],
                data["format"],
                times,
                data["year"],
            ])

    print(f"✔ Exported {len(grouped)} Rio rows → {filename}")


# ============================================================
# RUN
# ============================================================

if __name__ == "__main__":
    showtimes = scrape()
    rows = convert_showtimes_to_intervals_rows(showtimes)
    new_rows = incremental_save_and_backup(rows, "rio")

    print("\n".join(new_rows))

