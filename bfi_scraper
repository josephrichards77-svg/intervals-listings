import os
import csv
import traceback
import datetime
from pathlib import Path
from collections import defaultdict

import dateparser
from playwright.sync_api import sync_playwright
from playwright_stealth import stealth_sync

# ============================================================
# CONFIG
# ============================================================

CINEMA_NAME = "BFI Southbank"
INDEX_URL = "https://whatson.bfi.org.uk/Online/article/filmsindex"

DATE_FMT = "%Y-%m-%d"
TIME_FMT = "%H:%M"

# Output files
OUTPUT_FILE = "bfi_latest.csv"
BACKUP_FILE = "bfi_latest_backup.csv"

# ============================================================
# PREFIX CLEANING
# ============================================================

BFI_PREFIXES = [
    "BFI Flare:", "BFI Film Classics:", "BFI Cult:", "BFI Docs:",
    "Experimenta:", "Treasures:", "Preview:", "Re-release:",
    "Restoration:", "Silent Cinema:", "Women with a Movie Camera:",
    "Focus:", "Member exclusive:", "TV Preview:", "UK Premiere:",
    "Funday Preview:", "In Conversation:",
]

def strip_bfi_prefixes(title):
    t = title.strip()
    found = []
    changed = True
    while changed:
        changed = False
        for p in BFI_PREFIXES:
            if t.lower().startswith(p.lower()):
                found.append(p.rstrip(":"))
                t = t[len(p):].strip()
                changed = True
    return t, found

# ============================================================
# TMDB CACHE (YOUR EXISTING IMPLEMENTATION)
# ============================================================

# IMPORTANT — I’m preserving your existing method names;
# replace these stubs with your actual implementations.
from your_tmdb_module import init_db, tmdb_get_metadata, RELEASE_YEAR_RE

# ============================================================
# BACKUP SYSTEM
# ============================================================

def rotate_backups():
    """Move latest.csv → backup.csv before overwriting."""
    if os.path.exists(OUTPUT_FILE):
        try:
            os.replace(OUTPUT_FILE, BACKUP_FILE)
        except Exception:
            print("Warning: could not rotate backup.")

# ============================================================
# SCRAPER
# ============================================================

def scrape_bfi():
    showtimes = []
    today = datetime.date.today()

    conn = init_db()

    with sync_playwright() as p:
        browser = p.chromium.launch(
            headless=True,
            args=[
                "--disable-blink-features=AutomationControlled",
                "--disable-gpu",
                "--no-sandbox",
                "--disable-dev-shm-usage",
                "--disable-web-security",
                "--disable-features=IsolateOrigins,site-per-process",
                "--start-maximized",
            ]
        )

        page = browser.new_page()
        stealth_sync(page)

        print("Loading BFI index (headless stealth)…")

        try:
            page.goto(INDEX_URL, wait_until="networkidle", timeout=120000)
        except Exception as e:
            print(f"FATAL: Could not load BFI index: {e}")
            browser.close()
            conn.close()
            return []

        container = page.locator("div.Rich-text").first
        if container.count() == 0:
            print("Error: Could not find film list.")
            browser.close()
            conn.close()
            return []

        lis = container.locator("ul > li > a")
        total = lis.count()
        print(f"Found {total} films\n")

        for i in range(total):
            a = lis.nth(i)
            raw_title = a.inner_text().strip()
            href = a.get_attribute("href")

            if not href:
                continue
            if not href.startswith("https://"):
                href = "https://whatson.bfi.org.uk/Online/" + href.lstrip("/")

            # Prefix strip
            title, prefixes = strip_bfi_prefixes(raw_title)
            extra_context = "; ".join(prefixes)

            print(f"[{i+1}/{total}] {title}")

            film_page = browser.new_page()
            stealth_sync(film_page)

            try:
                film_page.goto(href, timeout=60000)

                article_context = film_page.evaluate(
                    "(() => (typeof articleContext !== 'undefined' ? articleContext : null))()"
                )
                if not article_context:
                    print("  ❌ No articleContext")
                    film_page.close()
                    continue

                search_names = article_context.get("searchNames") or []
                search_results = article_context.get("searchResults") or []
                if not search_names or not search_results:
                    print("  ❌ No searchResults")
                    film_page.close()
                    continue

                listings = [dict(zip(search_names, row)) for row in search_results]

                # Release year finder
                year_hint = None
                info_nodes = film_page.locator("p.Film-info__information__value").all_inner_texts()
                for info in info_nodes:
                    info = info.strip()
                    m = RELEASE_YEAR_RE.match(info)
                    if m:
                        year_hint = m.group("year")
                        break

                meta = tmdb_get_metadata(conn, title, year_hint)

                runtime_min = meta.get("runtime_min")
                director = meta.get("director") or ""
                final_year = meta.get("year") or (year_hint or "")

                default_format = "Digital"

                per_date = defaultdict(list)

                for row in listings:
                    start_raw = row.get("start_date") or row.get("startDate")
                    if not start_raw:
                        continue
                    dt = dateparser.parse(start_raw)
                    if not dt:
                        continue

                    date_str = dt.strftime(DATE_FMT)
                    date_obj = dt.date()

                    # INCREMENTAL FILTER: Only today → future
                    if date_obj < today:
                        continue

                    time_str = dt.strftime(TIME_FMT)
                    per_date[date_str].append(time_str)

                # Build showtime rows
                for date_str, times in per_date.items():
                    unique_times = sorted(set(times))
                    times_joined = ", ".join(unique_times)

                    showtimes.append(
                        {
                            "date": date_str,
                            "venue": CINEMA_NAME,
                            "title": title,
                            "director": director,
                            "runtime": f"{runtime_min} min" if runtime_min else "",
                            "format": default_format,
                            "time": times_joined,
                            "year": final_year,
                            "link": href,
                            "extra": extra_context
                        }
                    )

            except Exception as e:
                print(f"  Error on {raw_title}: {e}")
                traceback.print_exc()

            finally:
                film_page.close()

        browser.close()
        conn.close()

    return showtimes

# ============================================================
# SAVE CSV
# ============================================================

def save_csv(rows):
    rotate_backups()

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for s in rows:
            f.write(
                f"{s['date']};"
                f"{s['venue']};"
                f"<a href=\"{s['link']}\" target=\"_blank\">{s['title']}</a>;"
                f"{s['director']};"
                f"{s['runtime']};"
                f"{s['format']};"
                f"{s['time']};"
                f"{s['year']};"
                f"{s['extra']}\n"
            )

# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    rows = scrape_bfi()
    print(f"\nExtracted {len(rows)} BFI rows.")
    save_csv(rows)
    print(f"Saved to {OUTPUT_FILE}")
