from playwright.sync_api import sync_playwright
import dateparser
from collections import defaultdict
import requests
import sqlite3
from pathlib import Path
from datetime import datetime
import re

CINEMA_NAME = "BFI Southbank"

# ---------------------------------------------------------
#  TMDB API (shared with your other scrapers)
# ---------------------------------------------------------
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
DB_PATH = Path(__file__).with_name("tmdb_cache.db")
_MEM_CACHE = {}

RELEASE_YEAR_RE = re.compile(r"^[a-zA-Z -]+ (?P<year>(19\d\d)|2[012]\d\d)\..*$")

# ---------------------------------------------------------
#  DB INIT
# ---------------------------------------------------------
def init_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS tmdb_cache (
            title TEXT NOT NULL,
            year_hint TEXT,
            tmdb_id INTEGER,
            runtime_min INTEGER,
            director TEXT,
            final_year TEXT,
            last_updated TEXT,
            PRIMARY KEY (title, year_hint)
        )
        """
    )
    conn.commit()
    return conn


# ---------------------------------------------------------
#  TMDB LOOKUP
# ---------------------------------------------------------
def tmdb_get_metadata(conn, title: str, year_hint: str | None):
    key = (title, year_hint or "")

    if key in _MEM_CACHE:
        return _MEM_CACHE[key]

    cur = conn.execute(
        "SELECT runtime_min, director, final_year FROM tmdb_cache WHERE title = ? AND year_hint IS ?",
        (title, year_hint),
    )
    row = cur.fetchone()
    if row:
        runtime_min, director, final_year = row
        meta = {
            "runtime_min": runtime_min,
            "director": director,
            "year": final_year or year_hint,
        }
        _MEM_CACHE[key] = meta
        return meta

    if not TMDB_API_KEY:
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta

    try:
        params = {"api_key": TMDB_API_KEY, "query": title}
        if year_hint:
            params["year"] = year_hint

        r = requests.get("https://api.themoviedb.org/3/search/movie", params=params, timeout=10)
        r.raise_for_status()
        results = r.json().get("results") or []
        if not results:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        movie = results[0]
        if year_hint:
            for m in results:
                rd = (m.get("release_date") or "")[:4]
                if rd == str(year_hint):
                    movie = m
                    break

        movie_id = movie.get("id")
        if not movie_id:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        params = {"api_key": TMDB_API_KEY, "append_to_response": "credits"}
        r2 = requests.get(f"https://api.themoviedb.org/3/movie/{movie_id}", params=params, timeout=10)
        r2.raise_for_status()
        mdata = r2.json()

        runtime_min = mdata.get("runtime")

        release_date = mdata.get("release_date") or ""
        tmdb_year = release_date[:4] if release_date else None
        final_year = tmdb_year or year_hint

        director = None
        for c in (mdata.get("credits") or {}).get("crew") or []:
            if c.get("job") == "Director":
                director = c.get("name")
                break

        meta = {
            "runtime_min": runtime_min,
            "director": director,
            "year": final_year,
        }

        conn.execute(
            """
            INSERT OR REPLACE INTO tmdb_cache
            (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (title, year_hint, movie_id, runtime_min, director, final_year, datetime.utcnow().isoformat()),
        )
        conn.commit()

        _MEM_CACHE[key] = meta
        return meta

    except Exception:
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta



# ---------------------------------------------------------
#  SCRAPE FUNCTION (UNCHANGED)
# ---------------------------------------------------------
def scrape_bfi():
    INDEX_URL = "https://whatson.bfi.org.uk/Online/article/filmsindex"
    showtimes = []

    conn = init_db()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()

        print("Loading BFI films indexâ€¦")
        page.goto(INDEX_URL, timeout=60000)

        listings_container = page.locator("div.Rich-text").first
        if listings_container.count() == 0:
            print("Could not find films container.")
            browser.close()
            conn.close()
            return showtimes

        lis = listings_container.locator("ul > li > a")
        total = lis.count()
        print(f"Found {total} films\n")

        for i in range(total):
            a = lis.nth(i)
            title = a.inner_text().strip()
            href = a.get_attribute("href")

            if not href:
                continue
            if not href.startswith("https://"):
                href = "https://whatson.bfi.org.uk/Online/" + href.lstrip("/")

            print(f"[{i+1}/{total}] Fetching: {title}")

            film_page = browser.new_page()
            try:
                film_page.goto(href, timeout=60000)

                article_context = film_page.evaluate(
                    "(() => (typeof articleContext !== 'undefined' ? articleContext : null))()"
                )
                if not article_context:
                    film_page.close()
                    continue

                search_names = article_context.get("searchNames") or []
                search_results = article_context.get("searchResults") or []
                if not search_names or not search_results:
                    film_page.close()
                    continue

                listings = [dict(zip(search_names, row)) for row in search_results]

                year_hint = None
                info_nodes = film_page.locator("p.Film-info__information__value").all_inner_texts()
                for info in info_nodes:
                    m = RELEASE_YEAR_RE.match(info.strip())
                    if m:
                        year_hint = m.group("year")
                        break

                meta = tmdb_get_metadata(conn, title, year_hint)
                runtime_min = meta.get("runtime_min")
                director = meta.get("director") or ""
                final_year = meta.get("year") or (year_hint or "")

                default_format = "Digital"

                per_date = defaultdict(list)
                for listing in listings:
                    dt = dateparser.parse(listing.get("start_date") or listing.get("startDate"))
                    if not dt:
                        continue

                    date_str = dt.strftime("%Y-%m-%d")
                    time_str = dt.strftime("%H:%M")
                    per_date[date_str].append(time_str)

                for date_str, times in per_date.items():
                    unique_times = sorted(set(times))
                    times_joined = ", ".join(unique_times)

                    showtimes.append(
                        {
                            "date": date_str,
                            "venue": CINEMA_NAME,
                            "title": title,
                            "director": director,
                            "runtime": f"{runtime_min} min" if runtime_min else "",
                            "format": default_format,
                            "time": times_joined,
                            "year": final_year,
                            "link": href,
                        }
                    )

            except Exception as e:
                print(f"  Error on {title}: {e}")
            finally:
                film_page.close()

        browser.close()
        conn.close()

    return showtimes



# ---------------------------------------------------------
#  INTERVALS CLEAN-UP PACK
# ---------------------------------------------------------
TITLE_PREFIXES = [
    "Special Event:", "Made in Prague:", "BFI Presents:", "Q&A:",
    "ScreenTalk:", "Preview:", "Re-release:", "Double Bill:",
]

def clean_title(title):
    t = title.strip()
    for p in TITLE_PREFIXES:
        if t.lower().startswith(p.lower()):
            t = t[len(p):].strip()
    return t

def normalise_format(fmt):
    if not fmt:
        return "Digital"
    fmt = fmt.upper()
    if "35" in fmt:
        return "35mm"
    if "16" in fmt:
        return "16mm"
    if "70" in fmt:
        return "70mm"
    if "DCP" in fmt or "DIG" in fmt:
        return "DCP"
    return fmt

def clean_runtime(rt):
    if not rt:
        return ""
    m = re.search(r"(\d+)", str(rt))
    return m.group(1) if m else ""

def tidy_listing(s):
    return {
        "date": s["date"],
        "venue": s["venue"],
        "title": clean_title(s["title"]),
        "director": s["director"] or "",
        "runtime": clean_runtime(s["runtime"]),
        "format": normalise_format(s["format"]),
        "time": s["time"],
        "year": s["year"],
        "link": s["link"],
    }

def tidy_all(listings):
    return [tidy_listing(s) for s in listings]


# ---------------------------------------------------------
#  INCREMENTAL + DEDUPE + OUTPUT
# ---------------------------------------------------------
def filter_from_today(listings):
    today = datetime.today().date()
    out = []
    for s in listings:
        try:
            d = datetime.strptime(s["date"], "%Y-%m-%d").date()
            if d >= today:
                out.append(s)
        except:
            pass
    return out

def dedupe_listings(listings):
    seen = set()
    out = []
    for s in listings:
        key = (s["date"], s["title"], s["time"])
        if key not in seen:
            seen.add(key)
            out.append(s)
    return out

def print_semicolon_csv(listings):
    for s in listings:
        print(
            f"{s['date']};"
            f"{s['venue']};"
            f"<a href=\"{s['link']}\" target=\"_blank\">{s['title']}</a>;"
            f"{s['director']};"
            f"{s['runtime']};"
            f"{s['format']};"
            f"{s['time']};"
            f"{s['year']}"
        )


# ---------------------------------------------------------
#  MAIN
# ---------------------------------------------------------
if __name__ == "__main__":
    listings = scrape_bfi()

    listings = filter_from_today(listings)
    listings = dedupe_listings(listings)
    listings = tidy_all(listings)

    print("\n------ FINAL BFI LISTINGS ------\n")
    print_semicolon_csv(listings)
