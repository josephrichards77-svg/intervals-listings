import os
import datetime
from pathlib import Path
from collections import defaultdict
from zoneinfo import ZoneInfo

import dateparser
from playwright.sync_api import sync_playwright

from tmdb_cache import tmdb_get_metadata

# ============================================================
# CONFIG
# ============================================================

CINEMA_NAME = "The Garden Cinema"
BASE_URL = "https://www.thegardencinema.co.uk"

DATE_FMT = "%Y-%m-%d"
TIME_FMT = "%H:%M"

TXT_OUTPUT_FILE = "garden_existing.txt"

BACKUP_DIR = Path("backups/garden")
BACKUP_DIR.mkdir(parents=True, exist_ok=True)

# ============================================================
# HELPERS
# ============================================================

def normalize_url(href: str) -> str:
    if not href:
        return ""
    if href.startswith("http"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return BASE_URL + "/" + href


# ============================================================
# SCRAPER
# ============================================================

def scrape_garden():
    rows = []
    today = datetime.date.today()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        print("Loading Garden Cinema homepage…")
        page.goto(BASE_URL, timeout=60000)

        films = page.locator(".films-list__by-title__film")
        total = films.count()
        print(f"Found {total} films\n")

        for i in range(total):
            fd = films.nth(i)

            # Title + Link
            title_el = fd.locator(".films-list__by-title__film-title > a").first
            raw_title = title_el.inner_text().strip()
            title = raw_title  # We don’t strip rating here – TMDB search is robust

            href = title_el.get_attribute("href")
            link = normalize_url(href)

            print(f"[{i+1}/{total}] {title}")

            # Open film page
            fp = browser.new_page()
            fp.goto(link, timeout=60000)

            # Extract metadata
            description = fp.locator('meta[name="description"]').get_attribute("content") or ""
            season = ""

            if fp.locator(".film-detail__film__season-link a").count() > 0:
                season = fp.locator(".film-detail__film__season-link a").first.text_content().strip()

            # Screening panels
            screenings_root = fp.locator(".film-detail__screenings")
            panels = screenings_root.locator(".screening-panel")
            panel_count = panels.count()

            # TMDB backup metadata (runtime, director, year)
            tmdb_meta = tmdb_get_metadata(title)
            director = tmdb_meta["director"] or ""
            runtime_min = tmdb_meta["runtime_min"] or ""
            year = tmdb_meta["year"] or ""

        last_date_obj = None

for j in range(panel_count):
    panel = panels.nth(j)

    # Try to get date
    date_loc = panel.locator(".screening-panel__date-title")
    if date_loc.count() > 0:
        # New date header found
        date_str = date_loc.inner_text().strip()
        date_obj = dateparser.parse(date_str)
        if not date_obj:
            continue
        last_date_obj = date_obj.date()
    else:
        # No date found in this panel — use previous date
        if not last_date_obj:
            continue  # cannot assign a date
        date_obj = last_date_obj

    # Skip past dates
    if date_obj < today:
        continue

    # All time links within this panel
    time_links = panel.locator(".screening-time a.screening")
    tc = time_links.count()

    for k in range(tc):
        time_str = time_links.nth(k).inner_text().strip()

        dt = datetime.datetime.combine(
            date_obj,
            datetime.datetime.strptime(time_str, "%H:%M").time()
        )
        dt = dt.astimezone(ZoneInfo("Europe/London")).replace(tzinfo=None)

        rows.append({
            "date": dt.date().strftime(DATE_FMT),
            "venue": CINEMA_NAME,
            "title": title,
            "link": link,
            "director": director,
            "runtime": f"{runtime_min} min" if runtime_min else "",
            "format": "DCP",
            "time": time_str,
            "year": year,
            "extra": season,
        })


            fp.close()

        browser.close()

    rows.sort(key=lambda r: r["date"])
    return rows


# ============================================================
# SAVE TXT WITH BACKUPS
# ============================================================

def save_txt(rows):
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup_path = BACKUP_DIR / f"garden_{timestamp}.txt"

    # Backup previous file
    if os.path.exists(TXT_OUTPUT_FILE):
        try:
            os.replace(TXT_OUTPUT_FILE, backup_path)
            print(f"Backup created: {backup_path}")
        except Exception as e:
            print(f"[WARN] Could not create backup: {e}")

    # Write new file
    with open(TXT_OUTPUT_FILE, "w", encoding="utf-8") as f:
        for s in rows:
            f.write(
                f"{s['date']};"
                f"{s['venue']};"
                f"<a href=\"{s['link']}\" target=\"_blank\">{s['title']}</a>;"
                f"{s['director']};"
                f"{s['runtime']};"
                f"{s['format']};"
                f"{s['time']};"
                f"{s['year']};"
                f"{s['extra']}\n"
            )

    print(f"\nTXT saved to {TXT_OUTPUT_FILE}")


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    rows = scrape_garden()
    print(f"\nExtracted {len(rows)} Garden rows.")
    save_txt(rows)
