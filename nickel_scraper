import requests
from bs4 import BeautifulSoup
from datetime import datetime
import sqlite3
import re

BASE = "https://thenickel.co.uk"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
CACHE_DB = "tmdb_cache.db"


# ------------------------------------------------------------
# NEW: WEBSITE-FIRST METADATA EXTRACTION
# ------------------------------------------------------------
def extract_nickel_metadata(url: str) -> dict:
    """
    Scrape metadata from The Nickel's film page itself.
    Extracts director, year, runtime when available.

    TMDB will only fill missing fields.
    """
    try:
        r = requests.get(url, timeout=10)
        r.raise_for_status()
    except:
        return {}

    soup = BeautifulSoup(r.text, "html.parser")
    text = soup.get_text("\n", strip=True)

    out = {}

    # Example format: "(1978, UK, John Smith)"
    m = re.search(r"\((\d{4})\s*,[^,]+,\s*([^)]+)\)", text)
    if m:
        out["year"] = m.group(1)
        out["director"] = m.group(2).strip()

    # Runtime often appears like "100 mins"
    m = re.search(r"(\d{2,3})\s*mins?", text, re.I)
    if m:
        out["runtime"] = f"{m.group(1)}m"

    return out


# ------------------------------------------------------------
# TMDB LOOKUP (SAFE)
# ------------------------------------------------------------
def tmdb_lookup(title, year_hint):
    conn = sqlite3.connect(CACHE_DB)
    cur = conn.cursor()

    # 1. Try cache first
    cur.execute("""
        SELECT tmdb_id, runtime_min, director, final_year
        FROM tmdb_cache
        WHERE title = ? AND year_hint = ?
    """, (title, year_hint))

    row = cur.fetchone()
    if row:
        conn.close()
        tmdb_id, runtime, director, year = row
        return runtime, director, year

    # 2. Query TMDB API safely
    params = {"api_key": TMDB_API_KEY, "query": title}
    if year_hint:
        params["year"] = year_hint

    try:
        resp = requests.get("https://api.themoviedb.org/3/search/movie", params=params)
        data = resp.json()
    except Exception as e:
        print(f"[TMDB ERROR] Could not query TMDB for {title}: {e}")
        conn.close()
        return None, "", year_hint

    if not isinstance(data, dict) or "results" not in data or not data["results"]:
        conn.close()
        return None, "", year_hint

    movie = data["results"][0]
    tmdb_id = movie.get("id")
    year = (movie.get("release_date") or "0000")[:4]

    # Fetch detailed info
    try:
        detail = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}",
            params={"api_key": TMDB_API_KEY, "append_to_response": "credits"}
        ).json()
    except Exception as e:
        print(f"[TMDB ERROR] Could not fetch details for {title}: {e}")
        conn.close()
        return None, "", year

    runtime = detail.get("runtime")
    director = ""
    for c in detail.get("credits", {}).get("crew", []):
        if c.get("job") == "Director":
            director = c["name"]
            break

    # Write to cache
    cur.execute("""
        INSERT OR REPLACE INTO tmdb_cache
        (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (
        title, year_hint, tmdb_id, runtime, director, year,
        datetime.utcnow().isoformat()
    ))
    conn.commit()
    conn.close()

    return runtime, director, year


# ------------------------------------------------------------
# PARSING HELPERS
# ------------------------------------------------------------
def parse_date(d_str):
    """Convert DD.MM â†’ YYYY-MM-DD"""
    try:
        day, month = d_str.split(".")
        year = datetime.now().year
        return f"{year}-{month}-{day.zfill(2)}"
    except:
        return ""


def extract_year(text):
    """Extract (XXXX, ... ) patterns"""
    m = re.search(r"\((\d{4})", text)
    return m.group(1) if m else ""


# ------------------------------------------------------------
# SCRAPER CORE
# ------------------------------------------------------------
def scrape():
    html = requests.get(BASE).text
    soup = BeautifulSoup(html, "html.parser")

    blocks = soup.select("div[grid-row]")
    results = []

    # Cache per-URL (so each film page is fetched once)
    nickel_meta_cache = {}

    for blk in blocks:
        cols = blk.select("div[grid-col]")
        if len(cols) < 3:
            continue

        details = cols[1]   # middle
        meta = cols[2]      # right

        # ---------------------------------------------
        # TITLE
        # ---------------------------------------------
        title_el = details.find("b")
        if not title_el:
            continue
        title = title_el.text.strip()

        # ---------------------------------------------
        # YEAR + DIRECTOR (inline)
        # ---------------------------------------------
        raw_info_el = details.find("i")
        raw_info = raw_info_el.text.strip() if raw_info_el else ""

        year = extract_year(raw_info)

        director = ""
        if "," in raw_info:
            parts = raw_info.strip("()").split(",")
            director = parts[-1].strip()

        runtime = ""

        # ---------------------------------------------
        # FORMAT + DATE SECTION
        # ---------------------------------------------
        meta_text = meta.get_text(" ", strip=True)
        tokens = meta_text.split()

        format_text = tokens[0] if tokens else ""

        # Find date
        date_token = ""
        for t in tokens:
            if re.match(r"\d{2}\.\d{2}", t):
                date_token = t
                break
        date = parse_date(date_token)

        # Booking URL
        booking_link = ""
        a = meta.find("a", href=True)
        if a:
            booking_link = a["href"]

        # ---------------------------------------------
        # TIME
        # ---------------------------------------------
        show_time = ""
        times = details.find_all(string=re.compile(r"Film"))
        for t in times:
            m = re.search(r"Film\s+(\d{1,2}\.\d{2}(?:am|pm)?)", t, re.I)
            if m:
                raw = m.group(1).replace(".", ":")
                try:
                    if raw.lower().endswith(("am", "pm")):
                        dt = datetime.strptime(raw.lower(), "%I:%M%p")
                    else:
                        dt = datetime.strptime(raw, "%H:%M")
                    show_time = dt.strftime("%H:%M")
                except:
                    pass

        # ------------------------------------------------------------
        # WEBSITE-FIRST METADATA (NEW)
        # ------------------------------------------------------------
        if booking_link:
            full_url = booking_link if booking_link.startswith("http") else BASE + booking_link

            if full_url not in nickel_meta_cache:
                nickel_meta_cache[full_url] = extract_nickel_metadata(full_url)

            site_meta = nickel_meta_cache.get(full_url, {})

            if site_meta.get("director"):
                director = site_meta["director"]

            if site_meta.get("runtime"):
                runtime = site_meta["runtime"]

            if site_meta.get("year"):
                year = site_meta["year"]

        # ------------------------------------------------------------
        # TMDB FALLBACK (only fill missing)
        # ------------------------------------------------------------
        missing_meta = not (director and runtime and year)

        if missing_meta:
            tmdb_runtime, tmdb_director, tmdb_year = tmdb_lookup(title, year)

            if tmdb_runtime and not runtime:
                runtime = f"{tmdb_runtime}m"
            if tmdb_director and not director:
                director = tmdb_director
            if tmdb_year and not year:
                year = tmdb_year

        # ------------------------------------------------------------
        # Append result
        # ------------------------------------------------------------
        results.append({
            "date": date,
            "venue": "The Nickel",
            "title": title,
            "director": director,
            "runtime": runtime,
            "format": format_text,
            "time": show_time,
            "year": year,
            "url": booking_link
        })

    return results


# ------------------------------------------------------------
# OUTPUT
# ------------------------------------------------------------
if __name__ == "__main__":
    data = scrape()

    # Sort chronologically
    def sort_key(r):
        date = r["date"] or "9999-99-99"
        time = r["time"] or "00:00"
        return (date, time)

    data_sorted = sorted(data, key=sort_key)

    # Group by (date, title)
    grouped = {}
    for r in data_sorted:
        key = (r["date"], r["title"])
        if key not in grouped:
            grouped[key] = {
                "date": r["date"],
                "venue": r["venue"],
                "title": r["title"],
                "director": r["director"],
                "runtime": r["runtime"],
                "format": r["format"],
                "year": r["year"],
                "url": r["url"],
                "times": []
            }
        if r["time"]:
            grouped[key]["times"].append(r["time"])

    with open("nickel_output.txt", "w") as f:
        for k in sorted(grouped.keys()):
            g = grouped[k]
            time_str = ", ".join(g["times"])

            if g["url"]:
                title_html = f'<a href="{g["url"]}">{g["title"]}</a>'
            else:
                title_html = g["title"]

            f.write(
                f"{g['date']};"
                f"{g['venue']};"
                f"{title_html};"
                f"{g['director']};"
                f"{g['runtime']};"
                f"{g['format']};"
                f"{time_str};"
                f"{g['year']};\n"
            )

    print(f"Written {len(grouped)} grouped screenings to nickel_output.txt")
