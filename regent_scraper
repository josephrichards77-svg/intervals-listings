import os
import re
import json
from datetime import datetime

import dateparser
import requests
from playwright.sync_api import sync_playwright
from dataclasses import dataclass, field
import sqlite3

from intervals_cleaning import normalise_row, merge_rows, export_rows

# ================================================================
# CONFIG
# ================================================================

CINEMA_NAME = "Regent Street Cinema"
CINEMA_SHORTCODE = "RG"
BASE_URL = "https://www.regentstreetcinema.com"
LIST_URL = f"{BASE_URL}/now-playing"

OUTPUT_FILE = "regent_street_out.txt"
TMDB_DB_FILENAME = "tmdb_cache.db"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"

MOVIE_LINK_RE = re.compile(r"^https://www\.regentstreetcinema\.com/movie/.*$")

# ================================================================
# DATACLASS
# ================================================================

@dataclass
class ShowTime:
    cinema_shortcode: str
    title: str
    link: str
    datetime: datetime
    description: str = ""
    image_src: str = ""
    release_year: int | None = None
    categories: list[str] = field(default_factory=list)


# ================================================================
# TMDB CACHE + API
# ================================================================

_tmdb_conn = None


def get_tmdb_conn():
    """
    Shared TMDB cache connection. Expects a tmdb_cache table with:
      title TEXT,
      year_hint TEXT,
      tmdb_id INTEGER,
      runtime_min INTEGER,
      director TEXT,
      final_year TEXT,
      last_updated TEXT,
      PRIMARY KEY (title, year_hint)
    """
    global _tmdb_conn
    db_path = os.path.join(os.path.dirname(__file__), TMDB_DB_FILENAME)
    if not os.path.exists(db_path):
        return None
    if _tmdb_conn is None:
        _tmdb_conn = sqlite3.connect(db_path)
        _tmdb_conn.row_factory = sqlite3.Row
    return _tmdb_conn


def tmdb_cache_lookup(title: str, year_hint: int | None):
    conn = get_tmdb_conn()
    if conn is None:
        return {}

    cur = conn.cursor()

    if year_hint:
        cur.execute(
            """
            SELECT * FROM tmdb_cache
            WHERE LOWER(title)=LOWER(?) AND year_hint=?
            LIMIT 1
            """,
            (title, str(year_hint)),
        )
        row = cur.fetchone()
        if row:
            return dict(row)

    cur.execute(
        "SELECT * FROM tmdb_cache WHERE LOWER(title)=LOWER(?)",
        (title,),
    )
    row = cur.fetchone()
    return dict(row) if row else {}


def tmdb_cache_upsert(title, year_hint, tmdb_id, runtime, director, final_year):
    conn = get_tmdb_conn()
    if conn is None:
        return

    cur = conn.cursor()
    year_hint_s = str(year_hint) if year_hint else None
    final_year_s = str(final_year) if final_year else None

    cur.execute(
        """
        INSERT INTO tmdb_cache
            (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, datetime('now'))
        ON CONFLICT(title, year_hint) DO UPDATE SET
            tmdb_id=excluded.tmdb_id,
            runtime_min=excluded.runtime_min,
            director=excluded.director,
            final_year=excluded.final_year,
            last_updated=datetime('now')
        """,
        (title, year_hint_s, tmdb_id, runtime, director, final_year_s),
    )
    conn.commit()


def tmdb_fetch(title: str, year_hint: int | None):
    if not TMDB_API_KEY:
        return {}

    params = {"api_key": TMDB_API_KEY, "query": title}
    if year_hint:
        params["year"] = year_hint

    try:
        r = requests.get(
            "https://api.themoviedb.org/3/search/movie",
            params=params,
            timeout=10,
        ).json()
    except Exception:
        return {}

    results = r.get("results") or []
    if not results:
        return {}

    best = results[0]
    tmdb_id = best["id"]

    try:
        d = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}",
            params={"api_key": TMDB_API_KEY},
            timeout=10,
        ).json()
    except Exception:
        return {}

    runtime = d.get("runtime")
    release = d.get("release_date") or ""
    year = int(release[:4]) if len(release) >= 4 else None

    director = ""
    try:
        creds = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}/credits",
            params={"api_key": TMDB_API_KEY},
            timeout=10,
        ).json()
        for c in creds.get("crew", []):
            if c.get("job") == "Director":
                director = c.get("name")
                break
    except Exception:
        pass

    return {
        "tmdb_id": tmdb_id,
        "runtime": runtime,
        "director": director,
        "year": year,
    }


def lookup_tmdb(title: str, year_hint: int | None):
    """
    Cache-first TMDB lookup. year_hint can be None.
    Returns dict: director, runtime (str), year (str), format, country.
    """
    cached = tmdb_cache_lookup(title, year_hint)

    director = cached.get("director") or ""
    runtime = cached.get("runtime_min")
    final_year = cached.get("final_year")

    if not (director and runtime and final_year):
        api = tmdb_fetch(title, year_hint)
        if api:
            director = api.get("director") or director
            runtime = api.get("runtime") or runtime
            final_year = api.get("year") or final_year
            tmdb_cache_upsert(
                title,
                year_hint,
                api.get("tmdb_id"),
                runtime,
                director,
                final_year,
            )

    return {
        "director": director or "",
        "runtime": str(runtime) if runtime else "",
        "year": str(final_year) if final_year else "",
        "format": "DCP",
        "country": "",
    }


# ================================================================
# HELPERS
# ================================================================

def safe_meta(page, selector: str) -> str:
    el = page.locator(selector)
    if el.count() == 0:
        return ""
    try:
        return el.first.get_attribute("content") or ""
    except Exception:
        return ""


def extract_movie_links(html: str):
    hrefs = re.findall(r'href="([^"]+)"', html)
    return [h for h in hrefs if MOVIE_LINK_RE.match(h)]


def extract_categories(page) -> list[str]:
    """
    NOTES mode 3 (restricted):
    Only extract curated programme tags from `.categories a`.
    """
    cats = []
    nodes = page.locator(".categories a")
    for i in range(nodes.count()):
        txt = (nodes.nth(i).inner_text() or "").strip()
        if not txt:
            continue
        if txt not in cats:
            cats.append(txt)
    return cats


# ================================================================
# SCRAPER
# ================================================================

def scrape() -> list[ShowTime]:
    showtimes: list[ShowTime] = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        # JS disabled context as per your original scraper
        ctx = browser.new_context(java_script_enabled=False)
        page = ctx.new_page()
        page.goto(LIST_URL)

        html = page.locator("html").inner_html()
        links = extract_movie_links(html)

        for link in links:
            film = ctx.new_page()
            film.goto(link)

            title = safe_meta(film, "meta[property='og:title']")
            if title.endswith(" | Regent Street Cinema"):
                title = title.replace(" | Regent Street Cinema", "").strip()

            desc = safe_meta(film, "meta[property='og:description']")
            img = safe_meta(film, "meta[property='og:image']")

            # Categories / programme tags → NOTES
            categories = extract_categories(film)

            # Showtimes block
            show_blocks = film.locator(
                "//h1[normalize-space()='Showtimes']/following-sibling::h2"
            )

            for i in range(show_blocks.count()):
                raw = show_blocks.nth(i).text_content().strip()
                dt = dateparser.parse(raw)
                if not dt:
                    continue

                st = ShowTime(
                    cinema_shortcode=CINEMA_SHORTCODE,
                    title=title,
                    link=link,
                    datetime=dt,
                    description=desc,
                    image_src=img,
                    release_year=None,
                    categories=categories,
                )
                showtimes.append(st)

            film.close()

        browser.close()

    return showtimes


# ================================================================
# EXPORT VIA intervals_cleaning
# ================================================================

def export_with_cleaner(showtimes: list[ShowTime], filename: str = OUTPUT_FILE):
    """
    Build unified rows → normalise_row → merge_rows → export_rows.
    One row per (venue, date, title) with times merged (Mode A).
    """
    rows: list[dict] = []

    for s in showtimes:
        # TMDB cache-first metadata; year_hint not critical here
        tmdb = lookup_tmdb(s.title, s.release_year)

        raw_row = {
            "venue": CINEMA_NAME,
            "date": s.datetime.strftime("%Y-%m-%d"),
            "title": f"<a href='{s.link}'>{s.title}</a>",
            "director": tmdb["director"],
            "runtime_min": tmdb["runtime"],
            "format": tmdb["format"],
            "times": [s.datetime.strftime("%H:%M")],
            "year": tmdb["year"],
            "extra": ", ".join(s.categories) if s.categories else "",
            "url": s.link,
        }

        normal = normalise_row(raw_row)
        merge_rows(rows, normal)

    # Build final body (no header) from intervals_cleaning
    body = export_rows(rows)

    with open(filename, "w", encoding="utf-8") as f:
        # Fixed 9-column header
        f.write("DATE;VENUE;TITLE;DIRECTOR;RUNTIME;FORMAT;TIME;YEAR;NOTES\n")
        if body:
            f.write(body)
            if not body.endswith("\n"):
                f.write("\n")

    print(f"✔ Exported {len(rows)} merged Regent Street rows → {filename}")


# ================================================================
# MAIN
# ================================================================

if __name__ == "__main__":
    data = scrape()
    export_with_cleaner(data)
