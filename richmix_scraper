import datetime
import os
import re
import csv
import sqlite3
import argparse
import logging

import dateparser
import requests

from playwright.sync_api import sync_playwright
from dataclasses import dataclass, field

# ================================================================
# CONFIG
# ================================================================

CINEMA_NAME = "Rich Mix"
BASE_URL = "https://richmix.org.uk"
URL = f"{BASE_URL}/whats-on/cinema/"
TMDB_DB_FILENAME = "tmdb_cache.db"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"

HERE = os.path.dirname(__file__)
CACHE_FILE = os.path.join(HERE, "richmix_history.csv")
DEFAULT_EXPORT = os.path.join(HERE, "richmix_semicolon.txt")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

# ================================================================
# HELPERS
# ================================================================

def normalize_url(href: str) -> str:
    if not href:
        return ""
    if href.startswith("http://") or href.startswith("https://"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return BASE_URL + "/" + href

def clean_field(x):
    """Ensure CSV-friendly fields."""
    if x is None:
        return ""
    return str(x).strip()

def strip_title(title: str) -> str:
    """Remove BBFC ratings from the end."""
    rating_re = re.compile(r"\b(U|PG|12A|12|15|18)\b$", re.IGNORECASE)
    return rating_re.sub("", title).strip()


# ================================================================
# TMDB CACHE + API
# ================================================================

_tmdb_conn = None

def get_tmdb_conn():
    global _tmdb_conn
    path = os.path.join(HERE, TMDB_DB_FILENAME)
    if not os.path.exists(path):
        return None
    if _tmdb_conn is None:
        _tmdb_conn = sqlite3.connect(path)
        _tmdb_conn.row_factory = sqlite3.Row
    return _tmdb_conn

def tmdb_cache_lookup(title: str):
    conn = get_tmdb_conn()
    if conn is None:
        return {}
    cur = conn.cursor()
    cur.execute("SELECT * FROM tmdb_cache WHERE LOWER(title)=LOWER(?)", (title,))
    r = cur.fetchone()
    return dict(r) if r else {}

def tmdb_cache_upsert(title, tmdb_id, runtime, director, final_year):
    conn = get_tmdb_conn()
    if not conn:
        return

    ts = datetime.datetime.now(datetime.UTC).isoformat(timespec="seconds")
    cur = conn.cursor()

    cur.execute("""
        INSERT INTO tmdb_cache (title, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?)
        ON CONFLICT(title) DO UPDATE SET
            tmdb_id=excluded.tmdb_id,
            runtime_min=excluded.runtime_min,
            director=excluded.director,
            final_year=excluded.final_year,
            last_updated=excluded.last_updated
    """, (title, tmdb_id, runtime, director, final_year, ts))

    conn.commit()

def tmdb_fetch(title: str):
    if not TMDB_API_KEY:
        return {}

    try:
        r = requests.get(
            "https://api.themoviedb.org/3/search/movie",
            params={"api_key": TMDB_API_KEY, "query": title},
            timeout=10
        ).json()
        results = r.get("results", [])
    except:
        return {}

    if not results:
        return {}

    best = results[0]
    tmdb_id = best["id"]

    det = requests.get(
        f"https://api.themoviedb.org/3/movie/{tmdb_id}",
        params={"api_key": TMDB_API_KEY},
        timeout=10
    ).json()

    runtime = det.get("runtime")
    release_date = det.get("release_date", "")
    year = int(release_date[:4]) if len(release_date) >= 4 else ""

    director = ""
    try:
        credits = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}/credits",
            params={"api_key": TMDB_API_KEY},
            timeout=10
        ).json()
        for c in credits.get("crew", []):
            if c.get("job") == "Director":
                director = c["name"]
                break
    except:
        pass

    return {
        "tmdb_id": tmdb_id,
        "runtime": runtime,
        "director": director,
        "year": year,
    }

def lookup_tmdb(title: str):
    cached = tmdb_cache_lookup(title)

    director = cached.get("director") or ""
    runtime = cached.get("runtime_min")
    year = cached.get("final_year")

    if not director or not runtime or not year:
        api = tmdb_fetch(title)
        if api:
            director = api.get("director") or director
            runtime = api.get("runtime") or runtime
            year = api.get("year") or year
            tmdb_cache_upsert(title, api.get("tmdb_id"), runtime, director, year)

    return {
        "director": director or "",
        "runtime": str(runtime or ""),
        "year": year or "",
        "format": "DCP",
    }


# ================================================================
# NOTES EXTRACTION (Option D)
# ================================================================

def extract_notes_from_page(page):
    """
    Extract programme/season/series names only.
    Option D: Prefixed when useful, natural phrasing, no noise.
    """

    notes = []

    # 1. Extract category/tag labels
    cat_nodes = page.locator(".event-categories, .taxonomy-term")
    for i in range(cat_nodes.count()):
        txt = cat_nodes.nth(i).inner_text().strip()
        if not txt:
            continue

        # Ignore generic categories like "Cinema"
        if txt.lower() in ["cinema", "film"]:
            continue

        # Add prefixes only when useful
        if "club" in txt.lower():
            notes.append(f"Film Club: {txt}")
        elif "season" in txt.lower():
            notes.append(txt)  # already descriptive
        elif "festival" in txt.lower():
            notes.append(txt)
        else:
            # Series name like "Kids Kino Club"
            notes.append(txt)

    # 2. Title prefix pattern: "Film Club: NAME – Rich Mix"
    title_meta = page.locator('meta[property="og:title"]').get_attribute("content")
    if title_meta:
        if ":" in title_meta:
            possible = title_meta.split(":")[0].strip()
            if ("club" in possible.lower() or "season" in possible.lower()) and possible not in notes:
                notes.append(possible)

    # Return combined notes as a single string
    return ", ".join(notes).strip()


# ================================================================
# DATA CLASS
# ================================================================

@dataclass
class ShowTime:
    title: str
    link: str
    datetime: datetime.datetime
    img: str
    desc: str
    extra: dict = field(default_factory=dict)
    notes: str = ""


# ================================================================
# SCRAPER
# ================================================================

def scrape():
    out = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        page.goto(URL)
        articles = page.locator("section#articles article")

        for i in range(articles.count()):
            fd = articles.nth(i)

            raw_link = fd.locator("div.post-image a").get_attribute("href")
            link = normalize_url(raw_link)

            film = browser.new_page()
            film.goto(link)

            title = film.locator('meta[property="og:title"]').get_attribute("content")
            title = strip_title(title.replace(" - Rich Mix", "").strip())

            desc = film.locator('meta[property="og:description"]').get_attribute("content")

            tmdb = lookup_tmdb(title)
            notes = extract_notes_from_page(film)

            date_root = film.locator("#dates-and-times")
            if date_root.count() == 0:
                film.close()
                continue

            days = date_root.locator(".day")

            for j in range(days.count()):
                day = days.nth(j)
                date_text = day.locator(".weekday, .instance-date").inner_text()
                date_obj = dateparser.parse(date_text).date()

                times = day.locator(".times a.time")
                for k in range(times.count()):
                    t_text = times.nth(k).inner_text()
                    time_obj = dateparser.parse(t_text).time()
                    dt = datetime.datetime.combine(date_obj, time_obj)

                    out.append(
                        ShowTime(
                            title=title,
                            link=link,
                            datetime=dt,
                            img="",
                            desc=desc,
                            extra=tmdb,
                            notes=notes,
                        )
                    )

            film.close()

        browser.close()

    logging.info(f"Scraped {len(out)} Rich Mix showtimes")
    return out


# ================================================================
# INCREMENTAL CSV (9 columns)
# ================================================================

def load_existing():
    if not os.path.exists(CACHE_FILE):
        return {}
    existing = {}
    with open(CACHE_FILE, "r", encoding="utf-8") as f:
        reader = csv.reader(f, delimiter=";")
        next(reader, None)
        for row in reader:
            row = (row + [""] * 9)[:9]
            date, venue, title_html, director, runtime, fmt, time_str, year, notes = row
            key = (date, title_html, time_str)
            existing[key] = row
    return existing

def save_clean(rows):
    today = datetime.date.today()
    cleaned = []
    for row in rows:
        row = (row + [""] * 9)[:9]
        try:
            d = datetime.datetime.strptime(row[0], "%Y-%m-%d").date()
            if d >= today:
                cleaned.append(row)
        except:
            cleaned.append(row)

    cleaned.sort(key=lambda r: (r[0], r[2]))

    with open(CACHE_FILE, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter=";")
        writer.writerow([
            "DATE", "VENUE", "TITLE", "DIRECTOR",
            "RUNTIME", "FORMAT", "TIME", "YEAR", "NOTES"
        ])
        writer.writerows(cleaned)

    return cleaned


# ================================================================
# EXPORT (9 columns + NOTES + single-quote HTML)
# ================================================================

def export_to_semicolon(showtimes, filename=DEFAULT_EXPORT):
    grouped = {}

    for s in showtimes:
        d = s.datetime.strftime("%Y-%m-%d")
        t = s.datetime.strftime("%H:%M")
        key = (s.title, d)

        if key not in grouped:
            grouped[key] = {
                "title": s.title,
                "link": s.link,
                "date": d,
                "director": s.extra.get("director", ""),
                "runtime": s.extra.get("runtime", ""),
                "format": s.extra.get("format", "DCP"),
                "year": s.extra.get("year", ""),
                "notes": s.notes,
                "times": []
            }

        grouped[key]["times"].append(t)

    existing = load_existing()
    updated = dict(existing)

    for (title, date), data in grouped.items():
        safe_link = normalize_url(data["link"])
        title_html = f"<a href='{safe_link}'>{data['title']}</a>"

        for t in sorted(set(data["times"])):
            key = (date, title_html, t)
            row = [
                clean_field(date),
                clean_field(CINEMA_NAME),
                clean_field(title_html),
                clean_field(data["director"]),
                clean_field(data["runtime"]),
                clean_field(data["format"]),
                clean_field(t),
                clean_field(data["year"]),
                clean_field(data["notes"]),
            ]
            updated[key] = row

    cleaned_rows = save_clean(updated.values())

    with open(filename, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter=";")
        writer.writerow([
            "DATE", "VENUE", "TITLE", "DIRECTOR",
            "RUNTIME", "FORMAT", "TIME", "YEAR", "NOTES"
        ])
        writer.writerows(cleaned_rows)

    print(f"✔ Exported {len(cleaned_rows)} Rich Mix rows → {filename}")


# ================================================================
# MAIN
# ================================================================

def main():
    parser = argparse.ArgumentParser(description="Rich Mix scraper (with NOTES + 9 columns).")
    parser.add_argument("--clean-only", action="store_true")
    parser.add_argument("--outfile", default=DEFAULT_EXPORT)
    args = parser.parse_args()

    if args.clean_only:
        cleaned = save_clean(load_existing().values())
        with open(args.outfile, "w", encoding="utf-8", newline="") as f:
            writer = csv.writer(f, delimiter=";")
            writer.writerow([
                "DATE", "VENUE", "TITLE", "DIRECTOR",
                "RUNTIME", "FORMAT", "TIME", "YEAR", "NOTES"
            ])
            writer.writerows(cleaned)
        print("✔ Cache cleaned.")
        return

    showtimes = scrape()
    export_to_semicolon(showtimes, filename=args.outfile)


if __name__ == "__main__":
    main()
