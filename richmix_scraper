import datetime
import os
import re
import csv
import argparse
import logging

import dateparser
import requests

from playwright.sync_api import sync_playwright
from dataclasses import dataclass, field

# ================================================================
# CONFIG
# ================================================================

CINEMA_NAME = "Rich Mix"
BASE_URL = "https://richmix.org.uk"
URL = f"{BASE_URL}/whats-on/cinema/"

TMDB_DB_FILENAME = "tmdb_cache.db"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"

HERE = os.path.dirname(__file__)
DEFAULT_EXPORT = os.path.join(HERE, "richmix_semicolon.txt")

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

# ================================================================
# HELPERS
# ================================================================

def normalize_url(href: str) -> str:
    if not href:
        return ""
    if href.startswith("http://") or href.startswith("https://"):
        return href
    if href.startswith("/"):
        return BASE_URL + href
    return BASE_URL + "/" + href

def clean_field(x):
    if x is None:
        return ""
    return str(x).strip()

def strip_title(title: str) -> str:
    rating_re = re.compile(r"\b(U|PG|12A|12|15|18)\b$", re.IGNORECASE)
    return rating_re.sub("", title).strip()

# ================================================================
# TMDB CACHE + API
# ================================================================

_tmdb_conn = None

def get_tmdb_conn():
    global _tmdb_conn
    path = os.path.join(HERE, TMDB_DB_FILENAME)
    if not os.path.exists(path):
        return None
    if _tmdb_conn is None:
        _tmdb_conn = sqlite3.connect(path)
        _tmdb_conn.row_factory = sqlite3.Row
    return _tmdb_conn

def tmdb_cache_lookup(title: str):
    conn = get_tmdb_conn()
    if conn is None:
        return {}
    cur = conn.cursor()
    cur.execute("SELECT * FROM tmdb_cache WHERE LOWER(title)=LOWER(?)", (title,))
    r = cur.fetchone()
    return dict(r) if r else {}

def tmdb_cache_upsert(title, tmdb_id, runtime, director, final_year):
    conn = get_tmdb_conn()
    if not conn:
        return
    ts = datetime.datetime.now(datetime.UTC).isoformat(timespec="seconds")
    cur = conn.cursor()
    cur.execute("""
        INSERT INTO tmdb_cache (title, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?)
        ON CONFLICT(title) DO UPDATE SET
            tmdb_id=excluded.tmdb_id,
            runtime_min=excluded.runtime_min,
            director=excluded.director,
            final_year=excluded.final_year,
            last_updated=excluded.last_updated
    """, (title, tmdb_id, runtime, director, final_year, ts))
    conn.commit()

def tmdb_fetch(title: str):
    if not TMDB_API_KEY:
        return {}
    try:
        r = requests.get(
            "https://api.themoviedb.org/3/search/movie",
            params={"api_key": TMDB_API_KEY, "query": title},
            timeout=10
        ).json()
        results = r.get("results", [])
    except:
        return {}
    if not results:
        return {}
    best = results[0]
    tmdb_id = best["id"]
    det = requests.get(
        f"https://api.themoviedb.org/3/movie/{tmdb_id}",
        params={"api_key": TMDB_API_KEY},
        timeout=10
    ).json()
    runtime = det.get("runtime")
    release_date = det.get("release_date", "")
    year = int(release_date[:4]) if len(release_date) >= 4 else ""
    director = ""
    try:
        credits = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}/credits",
            params={"api_key": TMDB_API_KEY},
            timeout=10
        ).json()
        for c in credits.get("crew", []):
            if c.get("job") == "Director":
                director = c["name"]
                break
    except:
        pass
    return {
        "tmdb_id": tmdb_id,
        "runtime": runtime,
        "director": director,
        "year": year,
    }

def lookup_tmdb(title: str):
    cached = tmdb_cache_lookup(title)
    director = cached.get("director") or ""
    runtime = cached.get("runtime_min")
    year = cached.get("final_year")
    if not director or not runtime or not year:
        api = tmdb_fetch(title)
        if api:
            director = api.get("director") or director
            runtime = api.get("runtime") or runtime
            year = api.get("year") or year
            tmdb_cache_upsert(title, api.get("tmdb_id"), runtime, director, year)
    return {
        "director": director or "",
        "runtime": str(runtime or ""),
        "year": year or "",
        "format": "DCP",
    }

# ================================================================
# NOTES EXTRACTION
# ================================================================

def extract_notes_from_page(page):
    notes = []
    cat_nodes = page.locator(".event-categories, .taxonomy-term")
    for i in range(cat_nodes.count()):
        txt = cat_nodes.nth(i).inner_text().strip()
        if not txt:
            continue
        if txt.lower() in ["cinema", "film"]:
            continue
        notes.append(txt)
    return ", ".join(notes).strip()

# ================================================================
# DATA CLASS
# ================================================================

@dataclass
class ShowTime:
    title: str
    link: str
    datetime: datetime.datetime
    img: str
    desc: str
    extra: dict = field(default_factory=dict)
    notes: str = ""

# ================================================================
# SCRAPER
# ================================================================

def scrape():
    out = []
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        page.goto(URL)
        articles = page.locator("section#articles article")

        for i in range(articles.count()):
            fd = articles.nth(i)

            raw_link = fd.locator("div.post-image a").get_attribute("href")
            link = normalize_url(raw_link)

            film = browser.new_page()
            film.goto(link)

            raw_title = film.locator('meta[property="og:title"]').get_attribute("content") or ""
            raw_title = raw_title.replace(" - Rich Mix", "").strip()
            title = strip_title(raw_title).replace('"', "").replace("'", "").replace(";", ",").strip()

            desc = film.locator('meta[property="og:description"]').get_attribute("content")

            tmdb = lookup_tmdb(title)
            notes = extract_notes_from_page(film)

            date_root = film.locator("#dates-and-times")
            if date_root.count() == 0:
                film.close()
                continue

            days = date_root.locator(".day")
            for j in range(days.count()):
                day = days.nth(j)
                date_text = day.locator(".weekday, .instance-date").inner_text()
                date_obj = dateparser.parse(date_text).date()

                times = day.locator(".times a.time")
                for k in range(times.count()):
                    t_text = times.nth(k).inner_text()
                    time_obj = dateparser.parse(t_text).time()
                    dt = datetime.datetime.combine(date_obj, time_obj)

                    out.append(
                        ShowTime(
                            title=title,
                            link=link,
                            datetime=dt,
                            img="",
                            desc=desc,
                            extra=tmdb,
                            notes=notes,
                        )
                    )

            film.close()

        browser.close()
    logging.info(f"Scraped {len(out)} Rich Mix showtimes")
    return out

# ================================================================
# EXPORT — ALWAYS CLEAN (NO HISTORY)
# ================================================================

def export_to_semicolon(showtimes, filename=DEFAULT_EXPORT):
    grouped = {}

    for s in showtimes:
        d = s.datetime.strftime("%Y-%m-%d")
        t = s.datetime.strftime("%H:%M")
        key = (s.title, d)

        if key not in grouped:
            grouped[key] = {
                "title": s.title,
                "link": s.link,
                "date": d,
                "director": s.extra.get("director", ""),
                "runtime": s.extra.get("runtime", ""),
                "format": s.extra.get("format", "DCP"),
                "year": s.extra.get("year", ""),
                "notes": s.notes,
                "times": set(),
            }

        grouped[key]["times"].add(t)

    sorted_items = sorted(
        grouped.items(),
        key=lambda item: (item[1]["date"], item[1]["title"])
    )

    with open(filename, "w", encoding="utf-8", newline="") as f:
        writer = csv.writer(f, delimiter=";")

        writer.writerow([
            "DATE", "VENUE", "TITLE", "DIRECTOR",
            "RUNTIME", "FORMAT", "TIME", "YEAR", "NOTES"
        ])

        for (_, data) in sorted_items:
            safe_link = normalize_url(data["link"])
            title_html = f"<a href='{safe_link}'>{data['title']}</a>"

            for t in sorted(data["times"]):
                writer.writerow([
                    data["date"],
                    CINEMA_NAME,
                    title_html,
                    data["director"],
                    data["runtime"],
                    data["format"],
                    t,
                    data["year"],
                    data["notes"],
                ])

    print(f"✔ Exported Rich Mix rows → {filename}")

# ================================================================
# MAIN
# ================================================================

def main():
    parser = argparse.ArgumentParser(description="Rich Mix scraper (clean export).")
    parser.add_argument("--outfile", default=DEFAULT_EXPORT)
    args = parser.parse_args()

    showtimes = scrape()
    export_to_semicolon(showtimes, filename=args.outfile)

if __name__ == "__main__":
    main()
