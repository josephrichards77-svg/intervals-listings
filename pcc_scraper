from playwright.sync_api import sync_playwright
import dateparser
from collections import defaultdict
import requests
import sqlite3
from pathlib import Path
from datetime import datetime

from intervals_cleaning import normalise_row, merge_rows, export_rows


# ============================================================
#  PATHS â€” everything stored next to THIS file
# ============================================================

SCRIPT_DIR = Path(__file__).parent
EXISTING_PATH = SCRIPT_DIR / "pcc_existing.txt"
BACKUP_DIR = SCRIPT_DIR / "scraper_backups"
BACKUP_DIR.mkdir(exist_ok=True)

DB_PATH = SCRIPT_DIR / "tmdb_cache.db"


# ============================================================
#  TMDB API
# ============================================================

TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
_MEM_CACHE = {}


def init_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS tmdb_cache (
            title TEXT NOT NULL,
            year_hint TEXT,
            tmdb_id INTEGER,
            runtime_min INTEGER,
            director TEXT,
            final_year TEXT,
            last_updated TEXT,
            PRIMARY KEY (title, year_hint)
        )
        """
    )
    conn.commit()
    return conn


def tmdb_get_metadata(conn, title, year_hint):
    key = (title, year_hint or "")

    if key in _MEM_CACHE:
        return _MEM_CACHE[key]

    cur = conn.execute(
        "SELECT runtime_min, director, final_year FROM tmdb_cache WHERE title = ? AND year_hint IS ?",
        (title, year_hint),
    )
    row = cur.fetchone()
    if row:
        runtime_min, director, final_year = row
        meta = {
            "runtime_min": runtime_min,
            "director": director,
            "year": final_year or year_hint,
        }
        _MEM_CACHE[key] = meta
        return meta

    # TMDB API search
    try:
        params = {"api_key": TMDB_API_KEY, "query": title}
        if year_hint:
            params["year"] = year_hint

        r = requests.get(
            "https://api.themoviedb.org/3/search/movie",
            params=params,
            timeout=10,
        )
        r.raise_for_status()

        results = r.json().get("results") or []
        if not results:
            meta = {"runtime_min": "", "director": "", "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        movie = results[0]

        if year_hint:
            for m in results:
                if (m.get("release_date") or "")[:4] == str(year_hint):
                    movie = m
                    break

        movie_id = movie.get("id")

        # Get metadata
        r2 = requests.get(
            f"https://api.themoviedb.org/3/movie/{movie_id}",
            params={"api_key": TMDB_API_KEY, "append_to_response": "credits"},
            timeout=10,
        )
        r2.raise_for_status()
        mdata = r2.json()

        runtime_min = mdata.get("runtime") or ""
        final_year = (mdata.get("release_date") or "")[:4] or year_hint

        director = ""
        for c in mdata.get("credits", {}).get("crew", []):
            if c.get("job") == "Director":
                director = c.get("name")
                break

        meta = {
            "runtime_min": runtime_min,
            "director": director,
            "year": final_year,
        }

        conn.execute(
            """
            INSERT OR REPLACE INTO tmdb_cache
                (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (
                title, year_hint, movie_id,
                runtime_min, director, final_year,
                datetime.utcnow().isoformat(),
            ),
        )
        conn.commit()

        _MEM_CACHE[key] = meta
        return meta

    except:
        meta = {"runtime_min": "", "director": "", "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta


# ============================================================
# PCC SCRAPER
# ============================================================

def scrape_pcc():
    url = "https://princecharlescinema.com/whats-on/"
    conn = init_db()
    rows = []

    with sync_playwright() as p:
        page = p.chromium.launch(headless=True).new_page()
        page.goto(url, timeout=60000)
        page.wait_for_selector("div.film_list-outer")

        films = page.locator("div.film_list-outer")
        count = films.count()

        for i in range(count):
            f = films.nth(i)

            raw_title = f.locator("a.liveeventtitle").inner_text().strip()
            link = f.locator("a.liveeventtitle").get_attribute("href")

            # build title WITH a-href
            title_html = f'<a href="{link}">{raw_title}</a>'

            # year hint
            year_hint = None
            for s in f.locator("div.running-time span").all_inner_texts():
                s = s.strip()
                if s.isdigit() and len(s) == 4:
                    year_hint = s

            # projection format
            projection = ""
            for t in f.locator("span.tag").all_inner_texts():
                T = t.strip().upper()
                if any(fmt in T for fmt in ["35MM", "70MM", "16MM", "DCP", "DIGITAL"]):
                    projection = T
                    break

            # GROUP TIMES BY DATE
            perf = f.locator("ul.performance-list-items")
            items = perf.locator(":scope > *")

            grouped = defaultdict(list)
            cur_date = None

            for j in range(items.count()):
                el = items.nth(j)
                tag = el.evaluate("e => e.tagName.toLowerCase()")

                if tag == "div" and "heading" in (el.get_attribute("class") or ""):
                    cur_date = el.inner_text().strip()

                elif tag == "li" and cur_date:
                    t_el = el.locator("span.time")
                    if t_el.count():
                        t = t_el.inner_text().strip()
                        grouped[cur_date].append(t)

            # TMDB metadata
            meta = tmdb_get_metadata(conn, raw_title, year_hint)

            for date_str, times in grouped.items():
                times_sorted = sorted(times)
                dt = dateparser.parse(f"{date_str} {times_sorted[0]}")

                # handle year rollover
                if dt.month in (1, 2) and datetime.now().month in (11, 12):
                    dt = dt.replace(year=datetime.now().year + 1)

                date_out = dt.strftime("%Y-%m-%d")

                raw = {
                    "venue": "Prince Charles Cinema",
                    "date": date_out,
                    "title": title_html,
                    "director": meta.get("director"),
                    "runtime_min": meta.get("runtime_min"),
                    "format": projection,
                    "times": times_sorted,
                    "year": meta.get("year"),
                    "extra": "",
                    "url": link,
                }

                merge_rows(rows, normalise_row(raw))

    return rows


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    rows = scrape_pcc()

    text = export_rows(rows)
    new_lines = text.split("\n")

    # load existing
    try:
        existing = set(EXISTING_PATH.read_text().splitlines())
    except FileNotFoundError:
        existing = set()

    only_new = [l for l in new_lines if l not in existing]
    updated = existing.union(only_new)

    # save updated existing
    EXISTING_PATH.write_text("\n".join(sorted(updated)), encoding="utf-8")

    # save new backup
    ts = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup_file = BACKUP_DIR / f"pcc_{ts}.txt"
    backup_file.write_text("\n".join(only_new), encoding="utf-8")

    # print new rows for CMS
    print("\n".join(only_new))
