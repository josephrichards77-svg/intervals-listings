from playwright.sync_api import sync_playwright
import dateparser
import requests
import sqlite3
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import os

# Cleaner tools
from intervals_cleaning import normalise_row, merge_rows, export_rows


# ============================================================
# SETTINGS
# ============================================================

CINEMA_NAME = "Prince Charles Cinema"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"

DB_PATH = Path(__file__).with_name("tmdb_cache.db")
_MEM_CACHE = {}


# ============================================================
# PREFIX CLEANING (PCC)
# ============================================================

PCC_PREFIXES = [
    "PCC Presents:",
    "Late Night:",
    "Late Night 35mm:",
    "Cult Classics:",
    "Special Screening:",
    "Marathon:",
    "All-Nighter:",
    "Double Bill:",
    "Triple Bill:",
    "Mystery Movie:",
    "35mm:",
    "70mm:",
    "4K Restoration:",
]

def strip_pcc_prefixes(title):
    t = title.strip()
    found = []
    changed = True

    while changed:
        changed = False
        for p in PCC_PREFIXES:
            if t.lower().startswith(p.lower()):
                found.append(p.rstrip(":"))
                t = t[len(p):].strip()
                changed = True

    return t, found



# ============================================================
# TMDB CACHE
# ============================================================

def init_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute("""
        CREATE TABLE IF NOT EXISTS tmdb_cache (
            title TEXT NOT NULL,
            year_hint TEXT,
            tmdb_id INTEGER,
            runtime_min INTEGER,
            director TEXT,
            final_year TEXT,
            last_updated TEXT,
            PRIMARY KEY (title, year_hint)
        )
    """)
    conn.commit()
    return conn


# ============================================================
# TMDB METADATA FETCHER
# ============================================================

def tmdb_get_metadata(conn, title, year_hint):
    key = (title, year_hint or "")

    if key in _MEM_CACHE:
        return _MEM_CACHE[key]

    cur = conn.execute(
        "SELECT runtime_min, director, final_year FROM tmdb_cache WHERE title=? AND year_hint IS ?",
        (title, year_hint),
    )
    row = cur.fetchone()
    if row:
        runtime_min, director, final_year = row
        meta = {
            "runtime_min": runtime_min,
            "director": director,
            "year": final_year or year_hint
        }
        _MEM_CACHE[key] = meta
        return meta

    if not TMDB_API_KEY:
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta

    try:
        params = {"api_key": TMDB_API_KEY, "query": title}
        if year_hint:
            params["year"] = year_hint

        r = requests.get(
            "https://api.themoviedb.org/3/search/movie",
            params=params,
            timeout=10
        )
        r.raise_for_status()
        results = r.json().get("results") or []

        if not results:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        movie = results[0]

        if year_hint:
            for m in results:
                rd = (m.get("release_date") or "")[:4]
                if rd == str(year_hint):
                    movie = m
                    break

        movie_id = movie.get("id")
        if not movie_id:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        r2 = requests.get(
            f"https://api.themoviedb.org/3/movie/{movie_id}",
            params={"api_key": TMDB_API_KEY, "append_to_response": "credits"},
            timeout=10,
        )
        r2.raise_for_status()
        mdata = r2.json()

        runtime_min = mdata.get("runtime")
        release_date = mdata.get("release_date") or ""
        tmdb_year = release_date[:4] if release_date else None

        director = None
        for c in mdata.get("credits", {}).get("crew", []):
            if c.get("job") == "Director":
                director = c.get("name")
                break

        final_year = tmdb_year or year_hint

        meta = {
            "runtime_min": runtime_min,
            "director": director or "",
            "year": final_year or ""
        }

        conn.execute("""
            INSERT OR REPLACE INTO tmdb_cache
            (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        """, (
            title, year_hint, movie_id, runtime_min, director,
            final_year, datetime.utcnow().isoformat()
        ))
        conn.commit()

        _MEM_CACHE[key] = meta
        return meta

    except Exception:
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta



# ============================================================
# SCRAPER CORE
# ============================================================

def scrape_pcc():
    url = "https://princecharlescinema.com/whats-on/"
    conn = init_db()
    rows = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        print("Loading PCC...")
        page.goto(url, timeout=60000)
        page.wait_for_selector("div.film_list-outer")

        films = page.locator("div.film_list-outer")
        total = films.count()

        print(f"Found {total} films.\n")

        for i in range(total):
            print(f"[{i+1}/{total}] Extracting...")
            f = films.nth(i)

            title_raw = f.locator("a.liveeventtitle").inner_text().strip()
            url_raw = f.locator("a.liveeventtitle").get_attribute("href")

            # Clean PCC prefixes
            clean_title, prefixes = strip_pcc_prefixes(title_raw)

            # Year hint
            spans = f.locator("div.running-time span").all_inner_texts()
            year_hint = None
            for s in spans:
                s_clean = s.strip()
                if s_clean.isdigit() and len(s_clean) == 4:
                    year_hint = s_clean

            # Format
            tags = f.locator("span.tag").all_inner_texts()
            projection_format = ""
            for t in tags:
                t_up = t.upper()
                if any(x in t_up for x in ["35MM", "70MM", "16MM", "DCP", "4K", "DIGITAL"]):
                    projection_format = t_up
                    break

            # Times grouped by date
            perf = f.locator("ul.performance-list-items")
            children = perf.locator(":scope > *")

            grouped = defaultdict(list)
            current_date = None

            for j in range(children.count()):
                el = children.nth(j)
                tagname = el.evaluate("e => e.tagName.toLowerCase()")

                if tagname == "div":
                    if "heading" in (el.get_attribute("class") or ""):
                        current_date = el.inner_text().strip()

                elif tagname == "li" and current_date:
                    t_el = el.locator("span.time")
                    if t_el.count():
                        t = t_el.inner_text().strip()
                        if t:
                            grouped[current_date].append(t)

            if not grouped:
                continue

            # TMDB metadata
            meta = tmdb_get_metadata(conn, clean_title, year_hint)
            director = meta.get("director", "")
            runtime = meta.get("runtime_min") or ""
            year_final = meta.get("year") or (year_hint or "")

            # Build rows
            for date_str, times in grouped.items():
                times_sorted = sorted(times)

                dt = dateparser.parse(f"{date_str} {times_sorted[0]}")
                if not dt:
                    continue

                today = datetime.today()
                if dt.month in (1,2) and today.month in (11,12):
                    dt = dt.replace(year=today.year + 1)

                date_iso = dt.strftime("%Y-%m-%d")

                raw = {
                    "venue": CINEMA_NAME,
                    "date": date_iso,
                    "title": clean_title,  # clean title HERE
                    "director": director,
                    "runtime_min": runtime,
                    "format": projection_format,
                    "times": times_sorted,
                    "year": year_final,
                    "extra": "; ".join(prefixes),
                    "url": url_raw,
                }

                normal = normalise_row(raw)

                # Apply hyperlink AFTER normalise_row
                normal["title"] = (
                    f'<a href="{normal["url"]}" target="_blank">{clean_title}</a>'
                )

                merge_rows(rows, normal)

        browser.close()
        conn.close()

    return rows



# ============================================================
# MAIN OUTPUT + BACKUP
# ============================================================

def load_existing(path):
    if not Path(path).exists():
        return set()
    return {
        line.strip()
        for line in Path(path).read_text().splitlines()
        if line.strip()
    }


def save_existing(path, rows):
    with open(path, "w", encoding="utf-8") as f:
        for r in sorted(rows):
            f.write(r + "\n")


if __name__ == "__main__":
    rows = scrape_pcc()

    exported = export_rows(rows)
    lines = exported.split("\n")

    existing_path = Path(__file__).with_name("pcc_existing.txt")
    existing = load_existing(existing_path)

    new_rows = [r for r in lines if r not in existing]
    updated = existing.union(new_rows)
    save_existing(existing_path, updated)

    print("\n".join(new_rows))

    backup_dir = Path(__file__).with_name("pcc_backups")
    backup_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup_path = backup_dir / f"pcc_{timestamp}.txt"

    backup_path.write_text("\n".join(new_rows))

    print(f"\nBackup saved â†’ {backup_path}")
