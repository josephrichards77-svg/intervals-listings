from playwright.sync_api import sync_playwright
import dateparser
import requests
import sqlite3
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import os

# Cleaner tools
from intervals_cleaning import normalise_row, merge_rows, export_rows


# ============================================================
# SETTINGS
# ============================================================

CINEMA_NAME = "Prince Charles Cinema"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"

# Local DB cache next to this script
DB_PATH = Path(__file__).with_name("tmdb_cache.db")
_MEM_CACHE = {}   # in-memory TMDB cache


# ============================================================
# TMDB CACHE INITIALISATION
# ============================================================

def init_db():
    conn = sqlite3.connect(DB_PATH)
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS tmdb_cache (
            title TEXT NOT NULL,
            year_hint TEXT,
            tmdb_id INTEGER,
            runtime_min INTEGER,
            director TEXT,
            final_year TEXT,
            last_updated TEXT,
            PRIMARY KEY (title, year_hint)
        )
        """
    )
    conn.commit()
    return conn


# ============================================================
# TMDB METADATA FETCHER
# ============================================================

def tmdb_get_metadata(conn, title: str, year_hint: str | None):
    """
    Return dict: {runtime_min, director, year}
    Uses:
    1. in-memory cache
    2. sqlite cache
    3. TMDB API
    """

    key = (title, year_hint or "")

    # 1. Memory cache
    if key in _MEM_CACHE:
        return _MEM_CACHE[key]

    # 2. SQLite
    cur = conn.execute(
        "SELECT runtime_min, director, final_year FROM tmdb_cache WHERE title = ? AND year_hint IS ?",
        (title, year_hint),
    )
    row = cur.fetchone()
    if row:
        runtime_min, director, final_year = row
        meta = {
            "runtime_min": runtime_min,
            "director": director,
            "year": final_year or year_hint,
        }
        _MEM_CACHE[key] = meta
        return meta

    # 3. TMDB API
    if not TMDB_API_KEY:
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta

    try:
        params = {"api_key": TMDB_API_KEY, "query": title}
        if year_hint:
            params["year"] = year_hint

        r = requests.get(
            "https://api.themoviedb.org/3/search/movie",
            params=params,
            timeout=10
        )
        r.raise_for_status()
        results = r.json().get("results") or []

        if not results:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        movie = results[0]

        # Try match by year
        if year_hint:
            for m in results:
                rd = (m.get("release_date") or "")[:4]
                if rd == str(year_hint):
                    movie = m
                    break

        movie_id = movie.get("id")
        if not movie_id:
            meta = {"runtime_min": None, "director": None, "year": year_hint}
            _MEM_CACHE[key] = meta
            return meta

        # Get details
        r2 = requests.get(
            f"https://api.themoviedb.org/3/movie/{movie_id}",
            params={"api_key": TMDB_API_KEY, "append_to_response": "credits"},
            timeout=10,
        )
        r2.raise_for_status()
        mdata = r2.json()

        runtime_min = mdata.get("runtime")
        release_date = mdata.get("release_date") or ""
        tmdb_year = release_date[:4] if release_date else None

        director = None
        for c in mdata.get("credits", {}).get("crew", []):
            if c.get("job") == "Director":
                director = c.get("name")
                break

        final_year = tmdb_year or year_hint

        meta = {
            "runtime_min": runtime_min,
            "director": director or "",
            "year": final_year or "",
        }

        # Save
        conn.execute(
            """
            INSERT OR REPLACE INTO tmdb_cache
              (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
            VALUES (?, ?, ?, ?, ?, ?, ?)
            """,
            (
                title,
                year_hint,
                movie_id,
                runtime_min,
                director,
                final_year,
                datetime.utcnow().isoformat(),
            )
        )
        conn.commit()

        _MEM_CACHE[key] = meta
        return meta

    except Exception:
        meta = {"runtime_min": None, "director": None, "year": year_hint}
        _MEM_CACHE[key] = meta
        return meta


# ============================================================
# SCRAPER CORE
# ============================================================

def scrape_pcc():
    url = "https://princecharlescinema.com/whats-on/"
    conn = init_db()
    rows = []

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()

        print("Loading PCC...")
        page.goto(url, timeout=60000)
        page.wait_for_selector("div.film_list-outer")

        films = page.locator("div.film_list-outer")
        total = films.count()

        print(f"Found {total} films.\n")

        for i in range(total):
            print(f"[{i+1}/{total}] Extracting...")
            f = films.nth(i)

            title_raw = f.locator("a.liveeventtitle").inner_text().strip()
            url_raw = f.locator("a.liveeventtitle").get_attribute("href")

            # Year hint
            spans = f.locator("div.running-time span").all_inner_texts()
            year_hint = None
            for s in spans:
                s_clean = s.strip()
                if s_clean.isdigit() and len(s_clean) == 4:
                    year_hint = s_clean

            # Format
            tags = f.locator("span.tag").all_inner_texts()
            projection_format = ""
            for t in tags:
                t = t.strip().upper()
                if any(fmt in t for fmt in ["70MM", "35MM", "16MM", "DCP", "DIGITAL"]):
                    projection_format = t
                    break

            # Times grouped by date
            perf = f.locator("ul.performance-list-items")
            children = perf.locator(":scope > *")

            grouped = defaultdict(list)
            current_date = None

            for j in range(children.count()):
                el = children.nth(j)
                tagname = el.evaluate("e => e.tagName.toLowerCase()")

                if tagname == "div":
                    if "heading" in (el.get_attribute("class") or ""):
                        current_date = el.inner_text().strip()

                elif tagname == "li" and current_date:
                    t_el = el.locator("span.time")
                    if t_el.count():
                        t = t_el.inner_text().strip()
                        if t:
                            grouped[current_date].append(t)

            if not grouped:
                continue

            # TMDB metadata
            meta = tmdb_get_metadata(conn, title_raw, year_hint)
            director = meta.get("director", "")
            runtime = meta.get("runtime_min") or ""
            year_final = meta.get("year") or (year_hint or "")

            # Build rows
            for date_str, times in grouped.items():
                times_sorted = sorted(times)

                dt = dateparser.parse(f"{date_str} {times_sorted[0]}")
                if not dt:
                    continue

                # Year rollover
                today = datetime.today()
                if dt.month in (1, 2) and today.month in (11, 12):
                    dt = dt.replace(year=today.year + 1)

                date_iso = dt.strftime("%Y-%m-%d")

                # Create clickable title
                title_html = f'<a href="{url_raw}" target="_blank">{title_raw}</a>'

                raw = {
                    "venue": CINEMA_NAME,
                    "date": date_iso,
                    "title": title_html,
                    "director": director,
                    "runtime_min": runtime,
                    "format": projection_format,
                    "times": times_sorted,
                    "year": year_final,
                    "extra": "",
                    "url": url_raw,
                }

                normal = normalise_row(raw)
                merge_rows(rows, normal)

        browser.close()
        conn.close()

    return rows


# ============================================================
# INCREMENTAL + BACKUP
# ============================================================

def load_existing(path):
    if not Path(path).exists():
        return set()
    return set(
        line.strip()
        for line in open(path, "r", encoding="utf-8")
        if line.strip()
    )


def save_existing(path, rows):
    with open(path, "w", encoding="utf-8") as f:
        for r in sorted(rows):
            f.write(r + "\n")


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    rows = scrape_pcc()

    exported = export_rows(rows)
    lines = exported.split("\n")

    # Incremental
    existing_path = Path(__file__).with_name("pcc_existing.txt")
    existing = load_existing(existing_path)

    new_rows = [r for r in lines if r not in existing]
    updated = existing.union(new_rows)
    save_existing(existing_path, updated)

    # Print only NEW rows for CMS
    print("\n".join(new_rows))

    # Timestamped backup in same folder
    backup_dir = Path(__file__).with_name("pcc_backups")
    backup_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    backup_path = backup_dir / f"pcc_{timestamp}.txt"

    with open(backup_path, "w", encoding="utf-8") as f:
        f.write("\n".join(new_rows))

    print(f"\nBackup saved â†’ {backup_path}")
