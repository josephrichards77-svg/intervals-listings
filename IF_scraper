import requests
from bs4 import BeautifulSoup
from datetime import datetime
import sqlite3
import re
from difflib import SequenceMatcher
from urllib.parse import quote

# --------------------------------
# CONFIG
# --------------------------------

SAVOY_URL = (
    "https://cinelumiere.savoysystems.co.uk/"
    "CineLumiere.dll/TSelectItems.waSelectItemsPrompt.TcsWebMenuItem_600.TcsWebTab_601"
)

OUTPUT_FILE = "institut_francais.txt"
CACHE_DB = "tmdb_cache.db"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"


# ============================================================
# CLEANER — Fix capitalised titles
# ============================================================

def fix_title_caps(s):
    """
    Convert SHOUTING TITLES → Title Case, but keep known acronyms.
    """
    if not s:
        return s
    if s.isupper():
        return s.title()  # simple use-case works well here
    return s


# ============================================================
# TMDB HELPERS
# ============================================================

def normalize_title_for_match(title: str) -> str:
    t = title.lower()
    t = re.sub(r"[^a-z0-9\s]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def clean_base_title_for_tmdb(title_raw: str) -> str:
    t = re.split(r"\s*\(Cert.*", title_raw)[0]
    t = re.sub(r"\s+FFF\b", "", t, flags=re.IGNORECASE)
    t = re.sub(r"\s+\+\s*(Q&A|intro.*|introduction.*)$", "", t, flags=re.IGNORECASE)
    t = re.sub(r"\s+(part\s*[12]|p[12])$", "", t, flags=re.IGNORECASE)
    return t.strip()


def cache_lookup(title, year_hint):
    conn = sqlite3.connect(CACHE_DB)
    cur = conn.cursor()
    cur.execute("""
        SELECT tmdb_id, runtime_min, director, final_year
        FROM tmdb_cache
        WHERE title = ? AND (year_hint = ? OR (year_hint IS NULL AND ? IS NULL))
    """, (title.lower(), year_hint, year_hint))
    row = cur.fetchone()
    conn.close()

    if not row:
        return None

    tmdb_id, runtime_min, director, final_year = row
    return {
        "tmdb_id": tmdb_id,
        "runtime": runtime_min,
        "director": director,
        "year_final": final_year,
    }


def cache_store(title, year_hint, tmdb_id, runtime_min, director, final_year):
    conn = sqlite3.connect(CACHE_DB)
    cur = conn.cursor()
    cur.execute("""
        INSERT OR REPLACE INTO tmdb_cache
        (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (
        title.lower(),
        year_hint,
        tmdb_id,
        runtime_min,
        director,
        final_year,
        datetime.utcnow().isoformat()
    ))
    conn.commit()
    conn.close()


def tmdb_lookup(title, year_hint=None):
    params = {
        "api_key": TMDB_API_KEY,
        "query": title,
        "include_adult": "false",
    }
    if year_hint:
        params["year"] = year_hint

    try:
        r = requests.get("https://api.themoviedb.org/3/search/movie", params=params, timeout=10)
        r.raise_for_status()
    except:
        return None

    results = r.json().get("results", [])
    if not results:
        return None

    norm_query = normalize_title_for_match(title)

    best = None
    best_score = 0.0

    for res in results:
        tmdb_title = res.get("title", "") or ""
        norm_tmdb = normalize_title_for_match(tmdb_title)
        score = SequenceMatcher(None, norm_query, norm_tmdb).ratio()

        if score > best_score:
            best_score = score
            best = res

    if not best or best_score < 0.78:
        return None

    return {
        "tmdb_id": best.get("id"),
        "tmdb_title": best.get("title"),
        "tmdb_year": (best.get("release_date") or "")[:4],
    }


def tmdb_enrich(info):
    tmdb_id = info.get("tmdb_id")
    if not tmdb_id:
        return info

    enriched = dict(info)

    # DETAILS
    try:
        r = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}",
            params={"api_key": TMDB_API_KEY, "append_to_response": "credits"},
            timeout=10
        )
        d = r.json()
        enriched["runtime"] = d.get("runtime")
        enriched["year_final"] = (d.get("release_date") or "")[:4]
    except:
        pass

    # DIRECTOR
    try:
        crew = r.json().get("credits", {}).get("crew", [])
        dirs = [c["name"] for c in crew if c.get("job") == "Director"]
        if dirs:
            enriched["director"] = ", ".join(dirs)
    except:
        pass

    return enriched


def get_tmdb_metadata(raw_title, base_title, year_hint):
    # 1. cache
    cached = cache_lookup(base_title, year_hint)
    if cached:
        return cached

    # 2. TMDB search
    search = tmdb_lookup(base_title, year_hint)
    if not search and raw_title != base_title:
        search = tmdb_lookup(raw_title, year_hint)

    if not search:
        return None

    enriched = tmdb_enrich(search)

    # store
    cache_store(
        base_title,
        year_hint,
        enriched.get("tmdb_id"),
        enriched.get("runtime"),
        enriched.get("director"),
        enriched.get("year_final") or enriched.get("tmdb_year")
    )

    return {
        "tmdb_id": enriched.get("tmdb_id"),
        "runtime": enriched.get("runtime"),
        "director": enriched.get("director"),
        "year_final": enriched.get("year_final") or enriched.get("tmdb_year"),
    }


# ============================================================
# SCRAPER — NEW CORRECT VERSION
# ============================================================

def fetch_film_page(url):
    try:
        r = requests.get(url, timeout=15)
        r.raise_for_status()
        return r.text
    except:
        return ""


def extract_screenings_from_film(url):
    """
    Reads all <tr> rows at bottom of film page.
    Extracts correct per-row date + time.
    """
    html = fetch_film_page(url)
    if not html:
        return []

    soup = BeautifulSoup(html, "html.parser")

    rows = soup.select("table tr")
    screenings = []

    for tr in rows:
        date_el = tr.select_one("time.date")
        time_el = tr.select_one("time.time")

        if not date_el or not time_el:
            continue

        date_iso = date_el.get("datetime") or ""
        time_iso = time_el.get("datetime") or ""

        if not date_iso or not time_iso:
            continue

        date_str = date_iso.split("T")[0]
        time_str = time_el.text.strip()

        screenings.append((date_str, time_str))

    return screenings


def scrape():
    html = fetch_html()
    if not html:
        print("Could not fetch Savoy HTML.")
        return []

    soup = BeautifulSoup(html, "html.parser")
    headings = soup.find_all("h2")

    results = []

    for h in headings:
        a = h.find("a")
        if not a or not a.get("href"):
            continue

        title_raw = h.get_text(" ", strip=True)
        display_title = re.split(r"\s*\(Cert", title_raw)[0].strip()

        display_title = fix_title_caps(display_title)

        base_title = clean_base_title_for_tmdb(display_title)

        film_url = f"https://www.institut-francais.org.uk/cinema/{quote(base_title.lower().replace(' ', '-'))}/"

        year_guess = re.search(r"(\d{4})", title_raw)
        year_hint = year_guess.group(1) if year_guess else None

        # Website first — correct dates + times
        screenings = extract_screenings_from_film(film_url)

        # TMDB fallback
        meta = get_tmdb_metadata(display_title, base_title, year_hint) or {}

        director = meta.get("director", "")
        runtime = meta.get("runtime", "")
        year_final = meta.get("year_final", year_hint or "")

        for date_str, time_str in screenings:
            results.append({
                "date": date_str,
                "venue": "Institut français",
                "title": display_title,
                "director": director,
                "runtime": runtime,
                "format": "Digital",
                "time": time_str,
                "year": year_final,
                "url": film_url,
                "season": "",
            })

    return results


# ------------------------------------------------------------
# OUTPUT
# ------------------------------------------------------------

def export_txt(rows):
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for r in rows:
            href = f'<a href="{r["url"]}" target="_blank">{r["title"]}</a>'
            f.write(
                f"{r['date']};"
                f"{r['venue']};"
                f"{href};"
                f"{r['director']};"
                f"{r['runtime']};"
                f"{r['format']};"
                f"{r['time']};"
                f"{r['year']};"
                f"{r['season']}\n"
            )
    print(f"Saved {len(rows)} rows → {OUTPUT_FILE}")


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------

if __name__ == "__main__":
    rows = scrape()
    export_txt(rows)
