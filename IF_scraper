import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
import sqlite3
from urllib.parse import quote

BASE = "https://www.institut-francais.org.uk/cinema/"
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
CACHE_DB = "tmdb_cache.db"

OUTPUT_FILE = "institut_francais.txt"


# ------------------------------------------------------------
# TITLE CLEANING
# ------------------------------------------------------------
def clean_title(s):
    if not s:
        return ""
    s = s.replace("  ", " ").strip()
    return s


def normalise_for_match(s):
    s = s.lower()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


# ------------------------------------------------------------
# TMDB CACHE
# ------------------------------------------------------------
def tmdb_lookup(title, year_hint=None):
    conn = sqlite3.connect(CACHE_DB)
    c = conn.cursor()

    c.execute(
        "SELECT runtime_min, director, final_year FROM tmdb_cache WHERE title=? AND year_hint=?",
        (title, str(year_hint) if year_hint else None),
    )
    row = c.fetchone()
    if row:
        conn.close()
        return row[0], row[1], row[2]

    # API fallback
    try:
        params = {"api_key": TMDB_API_KEY, "query": title}
        r = requests.get("https://api.themoviedb.org/3/search/movie", params=params)
        data = r.json()
    except:
        conn.close()
        return None, "", year_hint

    if not data.get("results"):
        conn.close()
        return None, "", year_hint

    movie = data["results"][0]
    movie_id = movie.get("id")
    year = (movie.get("release_date") or "0000")[:4]

    # details
    detail = requests.get(
        f"https://api.themoviedb.org/3/movie/{movie_id}",
        params={"api_key": TMDB_API_KEY, "append_to_response": "credits"},
    ).json()

    runtime = detail.get("runtime")
    director = ""
    for crew in detail.get("credits", {}).get("crew", []):
        if crew.get("job") == "Director":
            director = crew.get("name")
            break

    # store
    c.execute(
        """
        INSERT OR REPLACE INTO tmdb_cache
        (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
        (
            title,
            str(year_hint) if year_hint else None,
            movie_id,
            runtime,
            director,
            year,
            datetime.utcnow().isoformat(),
        ),
    )
    conn.commit()
    conn.close()

    return runtime, director, year


# ------------------------------------------------------------
# FILM PAGE METADATA
# ------------------------------------------------------------
def get_film_page_metadata(url):
    try:
        html = requests.get(url).text
    except:
        return "", "", ""

    soup = BeautifulSoup(html, "html.parser")

    # director
    director = ""
    dir_el = soup.select_one(".film-details__meta-item--director")
    if dir_el:
        director = dir_el.get_text(" ", strip=True)
        director = re.sub(r"^Director\s*", "", director).strip()

    # runtime
    runtime = ""
    run_el = soup.select_one(".film-details__meta-item--duration")
    if run_el:
        txt = run_el.get_text(" ", strip=True)
        m = re.search(r"(\d+)\s*min", txt)
        if m:
            runtime = m.group(1)

    # year
    year = ""
    year_el = soup.select_one(".film-details__meta-item--year")
    if year_el:
        txt = year_el.get_text(" ", strip=True)
        m = re.search(r"(19\d{2}|20\d{2})", txt)
        if m:
            year = m.group(1)

    return director, runtime, year


# ------------------------------------------------------------
# PARSE MAIN LISTINGS PAGE
# ------------------------------------------------------------
def fetch_main_page():
    url = "https://www.institut-francais.org.uk/cinema/whats-on/"
    print("Fetching Institut Français main listings…")
    html = requests.get(url).text
    return BeautifulSoup(html, "html.parser")


def extract_films(soup):
    print("Extracting films…")

    films = []
    cards = soup.select(".film-card")

    print(f"→ Found {len(cards)} films")

    for card in cards:
        title_el = card.select_one(".film-card__title")
        if not title_el:
            continue

        title = clean_title(title_el.get_text(strip=True))

        link_el = card.find("a", href=True)
        if not link_el:
            continue

        url = link_el["href"]
        if not url.startswith("http"):
            url = BASE + url.lstrip("/")

        films.append((title, url))

    return films


# ------------------------------------------------------------
# PARSE PER-FILM SHOWTIMES TABLES
# ------------------------------------------------------------
def extract_showtimes(title, url):
    print(f" → {title}")

    try:
        html = requests.get(url).text
    except:
        return []

    soup = BeautifulSoup(html, "html.parser")

    rows = soup.select("table.film-performances-table tr")
    out = []

    for row in rows:
        date_el = row.select_one("time.date")
        time_el = row.select_one("time.time")

        if not date_el or not time_el:
            continue

        date_val = date_el.get("datetime", "")
        time_val = time_el.get("datetime", "")

        if not date_val or not time_val:
            continue

        date_str = date_val.strip()
        time_str = time_val.strip()

        out.append((date_str, time_str))

    return out


# ------------------------------------------------------------
# MAIN SCRAPER
# ------------------------------------------------------------
def scrape():
    soup = fetch_main_page()
    films = extract_films(soup)

    rows = []

    for title, url in films:
        showtimes = extract_showtimes(title, url)
        if not showtimes:
            continue

        # website metadata first
        director, runtime, year = get_film_page_metadata(url)

        # fallback to TMDB if missing
        if not runtime or not director or not year:
            tmdb_runtime, tmdb_dir, tmdb_year = tmdb_lookup(title, year)
            runtime = runtime or tmdb_runtime or ""
            director = director or tmdb_dir or ""
            year = year or tmdb_year or ""

        for date_str, time_str in showtimes:
            rows.append({
                "date": date_str,
                "venue": "Institut français",
                "title": title,
                "director": director,
                "runtime": runtime,
                "format": "Digital",
                "time": time_str,
                "year": year,
                "url": url,
                "season": "",
            })

    return rows


# ------------------------------------------------------------
# EXPORT
# ------------------------------------------------------------
def save(rows):
    rows = sorted(rows, key=lambda r: (r["date"], r["time"], r["title"]))

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for r in rows:
            title_html = f'<a href="{r["url"]}" target="_blank">{r["title"]}</a>'
            f.write(
                f'{r["date"]};'
                f'{r["venue"]};'
                f'{title_html};'
                f'{r["director"]};'
                f'{r["runtime"]};'
                f'{r["format"]};'
                f'{r["time"]};'
                f'{r["year"]};'
                f'{r["season"]}\n'
            )

    print(f"Saved → {OUTPUT_FILE} ({len(rows)} rows)")


# ------------------------------------------------------------
# MAIN
# ------------------------------------------------------------
if __name__ == "__main__":
    rows = scrape()
    save(rows)
