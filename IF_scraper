import requests
from bs4 import BeautifulSoup
from datetime import datetime
from urllib.parse import quote
import re
import sqlite3

# ============================================================
# CONFIG
# ============================================================

BASE_LIST = (
    "https://cinelumiere.savoysystems.co.uk/"
    "CineLumiere.dll/TSelectItems.waSelectItemsPrompt.TcsWebMenuItem_600.TcsWebTab_601"
)

BASE_FILM = "https://www.institut-francais.org.uk/cinema"

TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
CACHE_DB = "tmdb_cache.db"

OUTPUT_FILE = "institut_francais.txt"


# ============================================================
# NORMALISATION HELPERS
# ============================================================

def clean_title_for_slug(title: str) -> str:
    """Convert film title into Institut Français URL slug."""
    t = title.lower()
    t = re.sub(r"[^a-z0-9\s-]", "", t)
    t = re.sub(r"\s+", "-", t)
    return t.strip("-")


def normalize_title_for_match(t: str) -> str:
    t = t.lower()
    t = re.sub(r"[^a-z0-9\s]", " ", t)
    t = re.sub(r"\s+", " ", t)
    return t.strip()


def extract_year(text: str):
    """Extract first 4-digit year."""
    m = re.search(r"\b(19|20)\d{2}\b", text)
    return m.group(0) if m else None


def to_date(ds):
    """Convert yyyy-mm-dd to verify formatting."""
    try:
        return datetime.strptime(ds, "%Y-%m-%d").date()
    except:
        return None


# ============================================================
# TMDB LOOKUP / CACHE
# ============================================================

def cache_lookup(title, year_hint):
    conn = sqlite3.connect(CACHE_DB)
    cur = conn.cursor()
    cur.execute("""
        SELECT tmdb_id, runtime_min, director, final_year
        FROM tmdb_cache
        WHERE title=? AND (year_hint=? OR (year_hint IS NULL AND ? IS NULL))
    """, (title, year_hint, year_hint))
    row = cur.fetchone()
    conn.close()

    if not row:
        return None

    tmdb_id, runtime, director, final_year = row
    return {
        "tmdb_id": tmdb_id,
        "runtime": runtime,
        "director": director,
        "year_final": final_year
    }


def cache_store(title, year_hint, tmdb_id, runtime, director, year_final):
    conn = sqlite3.connect(CACHE_DB)
    cur = conn.cursor()
    cur.execute("""
        INSERT OR REPLACE INTO tmdb_cache
        (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, datetime('now'))
    """, (title, year_hint, tmdb_id, runtime, director, year_final))
    conn.commit()
    conn.close()


def tmdb_lookup(title, year_hint):
    """Queried only when cache fails."""
    params = {"api_key": TMDB_API_KEY, "query": title}
    if year_hint:
        params["year"] = year_hint

    try:
        resp = requests.get("https://api.themoviedb.org/3/search/movie", params=params, timeout=10)
        data = resp.json()
    except:
        return None

    results = data.get("results", [])
    if not results:
        return None

    best = results[0]
    return {
        "tmdb_id": best.get("id"),
        "tmdb_title": best.get("title"),
        "tmdb_year": (best.get("release_date") or "")[:4],
    }


def tmdb_enrich(info):
    tmdb_id = info.get("tmdb_id")
    if not tmdb_id:
        return info

    enriched = dict(info)

    # DETAILS
    try:
        d = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}",
            params={"api_key": TMDB_API_KEY},
            timeout=10
        ).json()

        enriched["runtime"] = d.get("runtime")
        enriched["year_final"] = (d.get("release_date") or "")[:4]
    except:
        pass

    # CREDITS
    try:
        c = requests.get(
            f"https://api.themoviedb.org/3/movie/{tmdb_id}/credits",
            params={"api_key": TMDB_API_KEY},
            timeout=10
        ).json()

        dirs = [x["name"] for x in c.get("crew", []) if x.get("job") == "Director"]
        if dirs:
            enriched["director"] = ", ".join(dirs)

    except:
        pass

    return enriched


def get_tmdb_metadata(raw_title, base_title, year_hint):
    """Full pipeline: cache → tmdb → enrich → cache_store."""
    cached = cache_lookup(base_title, year_hint)
    if cached:
        return cached

    search = tmdb_lookup(base_title, year_hint)
    if not search and raw_title != base_title:
        search = tmdb_lookup(raw_title, year_hint)

    if not search:
        return {}

    enriched = tmdb_enrich(search)

    cache_store(
        base_title,
        year_hint,
        enriched.get("tmdb_id"),
        enriched.get("runtime"),
        enriched.get("director"),
        enriched.get("year_final") or enriched.get("tmdb_year")
    )

    return {
        "runtime": enriched.get("runtime"),
        "director": enriched.get("director"),
        "year_final": enriched.get("year_final") or enriched.get("tmdb_year"),
    }


# ============================================================
# FETCH FUNCTIONS
# ============================================================

def fetch_html():
    """Fetch main Savoy (Ciné Lumière) page."""
    try:
        r = requests.get(BASE_LIST, timeout=20)
        r.raise_for_status()
        return r.text
    except Exception as e:
        print(f"[ERROR] Could not load main list: {e}")
        return ""


def fetch_film_page(url):
    """Fetch Institut Français film detail page."""
    try:
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        return r.text
    except:
        return ""


# ============================================================
# PARSING
# ============================================================

def parse_main_list(html):
    """Extract all films & generate film detail URLs."""
    soup = BeautifulSoup(html, "html.parser")

    films = []

    for h in soup.find_all("h2"):
        a = h.find("a")
        if not a or "TSelectItems" not in (a.get("href") or ""):
            continue

        raw_title = h.get_text(" ", strip=True)
        display_title = re.split(r"\s*\(Cert", raw_title)[0].strip()

        # Build Institut Français film URL
        slug = clean_title_for_slug(display_title)
        film_url = f"{BASE_FILM}/{quote(slug)}/"

        films.append({
            "title": display_title,
            "raw_title": raw_title,
            "url": film_url,
            "year_guess": extract_year(raw_title),
            "season": "French Film Festival London" if "FFF" in raw_title else ""
        })

    return films


def extract_screenings_from_film(html):
    soup = BeautifulSoup(html, "html.parser")

    rows = []
    table = soup.find("table")
    if not table:
        return rows

    for tr in table.find_all("tr"):
        date_el = tr.find("time", class_="date")
        time_el = tr.find("time", class_="time")

        if not date_el or not time_el:
            continue

        ds = date_el.get("datetime")  # YYYY-MM-DD
        ts = time_el.get("datetime")  # HH:MM

        if not ds or not ts:
            continue

        rows.append((ds, ts))

    return rows


# ============================================================
# SCRAPE CONTROLLER
# ============================================================

def scrape():
    html = fetch_html()
    films = parse_main_list(html)

    results = []

    for film in films:
        print("→", film["title"])

        film_html = fetch_film_page(film["url"])
        screenings = extract_screenings_from_film(film_html)

        # TMDB backup
        meta = get_tmdb_metadata(
            film["title"],
            film["title"],
            film["year_guess"]
        )

        director = meta.get("director", "")
        runtime = meta.get("runtime", "")
        year_final = meta.get("year_final", film["year_guess"] or "")

        for (ds, ts) in screenings:
            # Validate date
            if not to_date(ds):
                continue

            results.append({
                "date": ds,
                "venue": "Institut français",
                "title": film["title"],
                "url": film["url"],
                "director": director,
                "runtime": runtime,
                "format": "Digital",
                "time": ts,
                "year": year_final,
                "season": film["season"]
            })

    return results


# ============================================================
# EXPORT
# ============================================================

def export_txt(rows):
    rows_sorted = sorted(rows, key=lambda x: (x["date"], x["time"], x["title"]))

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for r in rows_sorted:
            title_html = f'<a href="{r["url"]}" target="_blank">{r["title"]}</a>'
            f.write(
                f'{r["date"]} ; '
                f'{r["venue"]} ; '
                f'{title_html} ; '
                f'{r["director"]} ; '
                f'{r["runtime"]} ; '
                f'{r["format"]} ; '
                f'{r["time"]} ; '
                f'{r["year"]} ; '
                f'{r["season"]}\n'
            )

    print("✔ Saved:", OUTPUT_FILE)


# ============================================================
# MAIN
# ============================================================

if __name__ == "__main__":
    rows = scrape()
    export_txt(rows)
