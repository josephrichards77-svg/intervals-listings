import requests
from bs4 import BeautifulSoup
from datetime import datetime
import re
import sqlite3
from urllib.parse import quote

# Base URL for constructing absolute links
BASE = "https://www.institut-francais.org.uk/cinema/"
# NOTE: You MUST replace this with your actual TMDB API key for live lookups to work.
TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
# Local SQLite database file for caching TMDB results
CACHE_DB = "tmdb_cache.db"

OUTPUT_FILE = "institut_francais.txt"


# ------------------------------------------------------------
# TITLE CLEANING
# ------------------------------------------------------------
def clean_title(s):
    """Removes leading/trailing spaces and replaces non-breaking spaces."""
    if not s:
        return ""
    # Note: \xa0 is the non-breaking space character often used in HTML.
    s = s.replace("  ", " ").strip()
    return s


def normalise_for_match(s):
    """Simplifies the title for fuzzy matching (currently unused in the main logic)."""
    s = s.lower()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s


# ------------------------------------------------------------
# TMDB CACHE SETUP
# ------------------------------------------------------------
def initialize_cache():
    """Ensures the SQLite database and the tmdb_cache table exist."""
    conn = sqlite3.connect(CACHE_DB)
    c = conn.cursor()
    c.execute("""
        CREATE TABLE IF NOT EXISTS tmdb_cache (
            title TEXT,
            year_hint TEXT,
            tmdb_id INTEGER,
            runtime_min INTEGER,
            director TEXT,
            final_year TEXT,
            last_updated TEXT,
            PRIMARY KEY (title, year_hint)
        )
    """)
    conn.commit()
    conn.close()

# ------------------------------------------------------------
# TMDB CACHE LOOKUP AND API FALLBACK
# ------------------------------------------------------------
def tmdb_lookup(title, year_hint=None):
    """
    Looks up movie details in the local cache or fetches them from the TMDB API.

    Returns: runtime (int/None), director (str), year (str)
    """
    initialize_cache()
    conn = sqlite3.connect(CACHE_DB)
    c = conn.cursor()

    # 1. Check Cache
    year_hint_str = str(year_hint) if year_hint else None
    c.execute(
        "SELECT runtime_min, director, final_year FROM tmdb_cache WHERE title=? AND year_hint=?",
        (title, year_hint_str),
    )
    row = c.fetchone()
    if row:
        conn.close()
        return row[0], row[1], row[2]

    # 2. API fallback (if not found in cache)
    try:
        # Step A: Search for the movie
        params = {"api_key": TMDB_API_KEY, "query": title}
        search_r = requests.get("https://api.themoviedb.org/3/search/movie", params=params)
        search_r.raise_for_status() # Raises an HTTPError for bad responses (4xx or 5xx)
        data = search_r.json()
    except requests.exceptions.RequestException as e:
        print(f"TMDB Search Error for {title}: {e}")
        conn.close()
        return None, "", year_hint # Return failure

    if not data.get("results"):
        conn.close()
        return None, "", year_hint

    movie = data["results"][0]
    movie_id = movie.get("id")
    year = (movie.get("release_date") or "0000")[:4]

    # Step B: Get detailed information (runtime, credits)
    try:
        detail_r = requests.get(
            f"https://api.themoviedb.org/3/movie/{movie_id}",
            params={"api_key": TMDB_API_KEY, "append_to_response": "credits"},
        )
        detail_r.raise_for_status()
        detail = detail_r.json()
    except requests.exceptions.RequestException as e:
        print(f"TMDB Detail Error for {title}: {e}")
        conn.close()
        return None, "", year

    runtime = detail.get("runtime")
    director = ""
    # Find the director in the crew list
    for crew in detail.get("credits", {}).get("crew", []):
        if crew.get("job") == "Director":
            director = crew.get("name")
            break

    # 3. Store in Cache
    c.execute(
        """
        INSERT OR REPLACE INTO tmdb_cache
        (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, ?)
        """,
        (
            title,
            year_hint_str,
            movie_id,
            runtime,
            director,
            year,
            datetime.utcnow().isoformat(),
        ),
    )
    conn.commit()
    conn.close()

    return runtime, director, year


# ------------------------------------------------------------
# FILM PAGE METADATA (Scrapes the individual film page)
# ------------------------------------------------------------
def get_film_page_metadata(url):
    """Extracts director, runtime, and year from the individual film detail page."""
    try:
        html = requests.get(url).text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching film page {url}: {e}")
        return "", "", ""

    soup = BeautifulSoup(html, "html.parser")

    director = ""
    dir_el = soup.select_one(".film-details__meta-item--director")
    if dir_el:
        director = dir_el.get_text(" ", strip=True)
        # Clean the "Director " prefix
        director = re.sub(r"^Director\s*", "", director).strip()

    runtime = ""
    run_el = soup.select_one(".film-details__meta-item--duration")
    if run_el:
        txt = run_el.get_text(" ", strip=True)
        # Extract digits followed by "min"
        m = re.search(r"(\d+)\s*min", txt)
        if m:
            runtime = m.group(1)

    year = ""
    year_el = soup.select_one(".film-details__meta-item--year")
    if year_el:
        txt = year_el.get_text(" ", strip=True)
        # Extract a 4-digit year (19xx or 20xx)
        m = re.search(r"(19\d{2}|20\d{2})", txt)
        if m:
            year = m.group(1)

    return director, runtime, year


# ------------------------------------------------------------
# PARSE MAIN LISTINGS PAGE
# ------------------------------------------------------------
def fetch_main_page():
    """Fetches and parses the main "What's On" cinema page."""
    url = "https://www.institut-francais.org.uk/cinema/whats-on/"
    print("Fetching Institut Français main listings…")
    try:
        html = requests.get(url).text
        return BeautifulSoup(html, "html.parser")
    except requests.exceptions.RequestException as e:
        print(f"Error fetching main page: {e}")
        return BeautifulSoup("", "html.parser") # Return empty soup on failure


def extract_films(soup):
    """Extracts film titles and URLs from the main listings page."""
    print("Extracting films…")

    films = []
    cards = soup.select(".film-card")

    print(f"→ Found {len(cards)} film cards")

    for card in cards:
        title_el = card.select_one(".film-card__title")
        if not title_el:
            continue

        title = clean_title(title_el.get_text(strip=True))

        link_el = card.find("a", href=True)
        if not link_el:
            continue

        url = link_el["href"]
        if not url.startswith("http"):
            # Construct absolute URL
            url = BASE + url.lstrip("/")

        films.append((title, url))

    return films


# ------------------------------------------------------------
# PARSE PER-FILM SHOWTIMES TABLES
# ------------------------------------------------------------
def extract_showtimes(title, url):
    """Extracts date and time strings from the showtimes table on the film's page."""
    print(f" → {title}")

    try:
        html = requests.get(url).text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching showtimes for {title}: {e}")
        return []

    soup = BeautifulSoup(html, "html.parser")

    # Select all table rows in the film performances table
    rows = soup.select("table.film-performances-table tr")
    out = []

    for row in rows:
        date_el = row.select_one("time.date")
        time_el = row.select_one("time.time")

        if not date_el or not time_el:
            continue

        # Get the machine-readable datetime attribute
        date_val = date_el.get("datetime", "")
        time_val = time_el.get("datetime", "")

        if not date_val or not time_val:
            continue

        date_str = date_val.strip()
        time_str = time_val.strip()

        out.append((date_str, time_str))

    return out


# ------------------------------------------------------------
# MAIN SCRAPER
# ------------------------------------------------------------
def scrape():
    """Coordinates the scraping process."""
    initialize_cache()
    soup = fetch_main_page()
    films = extract_films(soup)

    rows = []

    for title, url in films:
        showtimes = extract_showtimes(title, url)
        if not showtimes:
            continue

        # 1. Get website metadata first
        director, runtime, year = get_film_page_metadata(url)

        # 2. Fallback to TMDB if critical metadata is missing
        if not runtime or not director or not year:
            print(f"   (Missing data. Falling back to TMDB for: {title}...)")
            # Pass the year (even if partial) as a hint to TMDB lookup
            tmdb_runtime, tmdb_dir, tmdb_year = tmdb_lookup(title, year)

            # Use data from TMDB only if the website didn't have it
            runtime = runtime or tmdb_runtime or ""
            director = director or tmdb_dir or ""
            year = year or tmdb_year or ""

        if not director:
             print(f"   (Warning: No director found for {title})")

        # 3. Aggregate all showtimes with metadata
        for date_str, time_str in showtimes:
            rows.append({
                "date": date_str,
                "venue": "Institut français",
                "title": title,
                "director": director,
                "runtime": runtime,
                "format": "Digital",
                "time": time_str,
                "year": year,
                "url": url,
                "season": "",
            })

    return rows


# ------------------------------------------------------------
# EXPORT
# ------------------------------------------------------------
def save(rows):
    """Sorts the data and exports it to the specified text file."""
    # Sort by date, then time, then title
    rows = sorted(rows, key=lambda r: (r["date"], r["time"], r["title"]))

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        for r in rows:
            # Format the title as an HTML link
            title_html = f'<a href="{r["url"]}" target="_blank">{r["title"]}</a>'
            f.write(
                f'{r["date"]};'
                f'{r["venue"]};'
                f'{title_html};'
                f'{r["director"]};'
                f'{r["runtime"]};'
                f'{r["format"]};'
                f'{r["time"]};'
                f'{r["year"]};'
                f'{r["season"]}\n'
            )

    print(f"Saved → {OUTPUT_FILE} ({len(rows)} rows)")


# ------------------------------------------------------------
# MAIN EXECUTION
# ------------------------------------------------------------
if __name__ == "__main__":
    rows = scrape()
    save(rows)
