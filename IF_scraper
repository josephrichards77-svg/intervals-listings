import requests
import json
import os
import re
import sqlite3
from datetime import datetime
from difflib import SequenceMatcher
from urllib.parse import quote
from bs4 import BeautifulSoup
from collections import defaultdict

# --------------------------------
# CONFIG
# --------------------------------

SAVOY_URL = (
    "https://cinelumiere.savoysystems.co.uk/"
    "CineLumiere.dll/TSelectItems.waSelectItemsPrompt.TcsWebMenuItem_600.TcsWebTab_601"
)

DATA_FILE = "data_institut_francais.json"
OUTPUT_FILE = "institut_francais.txt"
CACHE_DB = "tmdb_cache.db"

TMDB_API_KEY = "63ca1bc21617054972b7bc2c64db096b"
TMDB_SEARCH_URL = "https://api.themoviedb.org/3/search/movie"


# --------------------------------
# NORMALISATION HELPERS
# --------------------------------

def normalize(s):
    return re.sub(r"[^a-z0-9\s]", "", s.lower()).strip()


def clean_base_title_for_tmdb(title_raw: str) -> str:
    t = re.split(r"\s*\(Cert.*", title_raw)[0]
    t = re.sub(r"\s+FFF\b", "", t, flags=re.IGNORECASE)
    t = re.sub(r"\s+\+\s*(Q&A|intro.*|introduction.*)$", "", t, flags=re.IGNORECASE)
    return t.strip()


def extract_year(text: str):
    if not text:
        return None
    m = re.search(r"\((\d{4})", text)
    return m.group(1) if m else None


def make_institut_url(display_title: str) -> str:
    slug = display_title.lower()
    slug = re.sub(r"[^a-z0-9\s-]", "", slug)
    slug = re.sub(r"\s+", "-", slug).strip("-")
    return f"https://www.institut-francais.org.uk/cinema/{quote(slug)}/"


# --------------------------------
# TMDB CACHE FUNCTIONS
# --------------------------------

def cache_lookup(title, year_hint):
    conn = sqlite3.connect(CACHE_DB)
    cur = conn.cursor()
    cur.execute("""
        SELECT tmdb_id, runtime_min, director, final_year
        FROM tmdb_cache
        WHERE title = ? AND (year_hint = ? OR (year_hint IS NULL AND ? IS NULL))
    """, (title, year_hint, year_hint))
    row = cur.fetchone()
    conn.close()
    if not row:
        return None
    tmdb_id, runtime_min, director, final_year = row
    return {
        "tmdb_id": tmdb_id,
        "runtime": runtime_min,
        "director": director,
        "year_final": final_year,
    }


def cache_store(title, year_hint, tmdb_id, runtime_min, director, final_year):
    conn = sqlite3.connect(CACHE_DB)
    cur = conn.cursor()
    cur.execute("""
        INSERT OR REPLACE INTO tmdb_cache
        (title, year_hint, tmdb_id, runtime_min, director, final_year, last_updated)
        VALUES (?, ?, ?, ?, ?, ?, ?)
    """, (
        title, year_hint, tmdb_id, runtime_min, director, final_year,
        datetime.utcnow().isoformat()
    ))
    conn.commit()
    conn.close()


# --------------------------------
# TMDB LOOKUP
# --------------------------------

def tmdb_lookup(title, year_hint=None):
    params = {"api_key": TMDB_API_KEY, "query": title}
    if year_hint:
        params["year"] = year_hint

    r = requests.get(TMDB_SEARCH_URL, params=params, timeout=10)
    js = r.json()

    if "results" not in js or not js["results"]:
        return None

    q = normalize(title)
    best = None
    best_score = 0

    for res in js["results"]:
        t = normalize(res.get("title", ""))
        score = SequenceMatcher(None, q, t).ratio()

        release_year = (res.get("release_date") or "")[:4]
        if year_hint and release_year != str(year_hint):
            continue

        if score > best_score:
            best_score = score
            best = res

    if not best:
        return None

    return {
        "tmdb_id": best["id"],
        "tmdb_title": best["title"],
        "tmdb_year": (best.get("release_date") or "")[:4],
    }


def tmdb_enrich(info):
    tmdb_id = info.get("tmdb_id")
    if not tmdb_id:
        return info

    enriched = dict(info)

    d = requests.get(
        f"https://api.themoviedb.org/3/movie/{tmdb_id}",
        params={"api_key": TMDB_API_KEY, "append_to_response": "credits"},
        timeout=10
    ).json()

    enriched["runtime"] = d.get("runtime")
    enriched["year_final"] = (d.get("release_date") or "")[:4]

    directors = [c["name"] for c in d.get("credits", {}).get("crew", []) if c.get("job") == "Director"]
    if directors:
        enriched["director"] = ", ".join(directors)

    return enriched


def get_tmdb_metadata(raw_title, base_title, year_hint):
    cached = cache_lookup(base_title, year_hint)
    if cached:
        return cached

    search = tmdb_lookup(base_title, year_hint)
    if not search and raw_title != base_title:
        search = tmdb_lookup(raw_title, year_hint)

    if not search:
        return None

    enriched = tmdb_enrich(search)

    cache_store(
        base_title,
        year_hint,
        enriched.get("tmdb_id"),
        enriched.get("runtime"),
        enriched.get("director"),
        enriched.get("year_final") or enriched.get("tmdb_year")
    )

    return {
        "tmdb_id": enriched.get("tmdb_id"),
        "runtime": enriched.get("runtime"),
        "director": enriched.get("director"),
        "year_final": enriched.get("year_final") or enriched.get("tmdb_year"),
    }


# --------------------------------
# DATE/TIME PARSING (FIXED!)
# --------------------------------

DATE_RE = re.compile(r"([A-Za-z]+ \d{1,2} [A-Za-z]{3} \d{4})")
TIME_RE = re.compile(r"^\d{1,2}:\d{2}$")


def extract_one_block(block):
    """
    Returns:
        {
          "date": "YYYY-MM-DD",
          "times": ["HH:MM", ...]
        }
    or None
    """

    text = block.get_text(" ", strip=True)

    # Extract *this block’s date*, not a previously seen one
    m = DATE_RE.search(text)
    if not m:
        return None

    date_str = m.group(1)

    for fmt in ("%A %d %b %Y", "%a %d %b %Y"):
        try:
            dt = datetime.strptime(date_str, fmt)
            break
        except:
            dt = None

    if not dt:
        return None

    date_out = dt.strftime("%Y-%m-%d")

    # Extract the TIME values in THIS SAME BLOCK
    times = []
    for a in block.find_all("a"):
        t = a.get_text(strip=True)
        if TIME_RE.match(t):
            times.append(t)

    return {
        "date": date_out,
        "times": times,
    }


# --------------------------------
# FETCH + PARSE SAVOY HTML
# --------------------------------

def fetch_html():
    try:
        r = requests.get(SAVOY_URL, headers={"User-Agent": "Mozilla/5.0"}, timeout=20)
        r.raise_for_status()
        return r.text
    except:
        return ""


def parse(html):
    screenings = []
    if not html:
        return screenings

    soup = BeautifulSoup(html, "html.parser")

    headings = soup.find_all("h2")

    for h in headings:
        a = h.find("a")
        if not a or "TSelectItems" not in (a.get("href") or ""):
            continue

        title_raw = h.get_text(" ", strip=True)
        display_title = re.split(r"\s*\(Cert.*", title_raw)[0].strip()
        base_title = clean_base_title_for_tmdb(display_title)
        year_guess = extract_year(title_raw)
        season = "French Film Festival London" if "FFF" in title_raw else ""

        url = make_institut_url(display_title)

        # NEW: parse ONLY until next <h2>
        sib = h.next_sibling
        while sib and not (hasattr(sib, "name") and sib.name == "h2"):

            if hasattr(sib, "name") and sib.name in ("p", "div", "li", "span"):

                block_info = extract_one_block(sib)

                if block_info:
                    date = block_info["date"]
                    for t in block_info["times"]:
                        screenings.append({
                            "title": display_title,
                            "tmdb_title_base": base_title,
                            "date": date,
                            "time": t,
                            "url": url,
                            "venue": "Institut français",
                            "year_guess": year_guess,
                            "season": season,
                            "director": "",
                            "runtime": "",
                            "format": "Digital",
                        })

            sib = sib.next_sibling

    return screenings


# --------------------------------
# EXPORT — ONE ROW PER FILM PER DAY
# --------------------------------

def export_txt(data, path=OUTPUT_FILE):
    final = defaultdict(lambda: {"times": []})

    for s in data:
        key = (
            s["date"],
            s["title"],
            s["venue"],
            s.get("director", ""),
            s.get("runtime", ""),
            s.get("format", ""),
            s.get("year_final", s.get("year_guess", "")),
            s.get("season", ""),
        )

        out = final[key]
        out["date"] = s["date"]
        out["title"] = s["title"]
        out["venue"] = s["venue"]
        out["director"] = s.get("director", "")
        out["runtime"] = s.get("runtime", "")
        out["format"] = s.get("format", "")
        out["year"] = s.get("year_final", s.get("year_guess", ""))
        out["season"] = s.get("season", "")
        out["url"] = s["url"]
        out["times"].append(s["time"])

    rows = []
    for key, g in sorted(final.items(), key=lambda x: (x[1]["date"], x[1]["title"])):
        times = ", ".join(sorted(set(g["times"])))
        href = f'<a href="{g["url"]}" target="_blank">{g["title"]}</a>'

        rows.append(
            f"{g['date']}; Institut français; {href}; {g['director']}; "
            f"{g['runtime']}; {g['format']}; {times}; {g['year']}; {g['season']}"
        )

    with open(path, "w", encoding="utf-8") as f:
        f.write("\n".join(rows))

    print(f"Saved semicolon export → {path}")


# --------------------------------
# MAIN
# --------------------------------

def scrape_institut_francais():
    html = fetch_html()
    parsed = parse(html)

    # Enrich
    for s in parsed:
        info = get_tmdb_metadata(s["title"], s["tmdb_title_base"], s["year_guess"])
        if info:
            s.update(info)

    export_txt(parsed)
    return parsed


if __name__ == "__main__":
    scrape_institut_francais()
